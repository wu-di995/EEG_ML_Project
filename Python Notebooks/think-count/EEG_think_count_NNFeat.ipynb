{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features using neural networks\n",
    "\n",
    "Pass each window 10 times and get the last layer parameters\n",
    "Each channel will have 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pickle as pkl\n",
    "import itertools \n",
    "import glob\n",
    "from sklearn import svm \n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "%matplotlib inline \n",
    "# %matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(3840, 32)\n(3840, 32)\n26.553326000000002\n26.553326000000002\n"
    }
   ],
   "source": [
    "# Import raw datasets, non-windowed \n",
    "think_df = pd.read_pickle(\"F:\\EEG-data\\\\think-count\\\\raw/think.pkl\")\n",
    "count_df = pd.read_pickle(\"F:\\EEG-data\\\\think-count\\\\raw/think.pkl\")\n",
    "\n",
    "print(think_df.shape)\n",
    "print(count_df.shape)\n",
    "print(think_df.iloc[0,0]) # Each element in dataframe is a single timestep\n",
    "print(count_df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# # Testing on a single column \n",
    "testcol = think_df.iloc[:,0].values\n",
    "\n",
    "# # Normalize column \n",
    "testcol = keras.utils.normalize(testcol).flatten()\n",
    "\n",
    "time = np.linspace(0,10,len(testcol))\n",
    "split_time = int(len(testcol)*0.8)\n",
    "time_train = time[:split_time]\n",
    "x_train = testcol[:split_time]\n",
    "time_valid = time[split_time:]\n",
    "x_valid = testcol[split_time:]\n",
    "\n",
    "window_size = 12\n",
    "# batch_size = 32\n",
    "# shuffle_buffer_size = 1000\n",
    "\n",
    "def windowed_dataset(dataframe,window_size):\n",
    "    N,M = dataframe.shape\n",
    "    ds_labels = np.zeros((N-window_size+1,1,M))\n",
    "    ds_windows = np.zeros((N-window_size+1,window_size,M))\n",
    "    for i in range(M):\n",
    "        channel = dataframe.iloc[:,i].values\n",
    "        channel = keras.utils.normalize(channel).flatten()\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(channel)\n",
    "        win_dataset = dataset.window(window_size+1,1,drop_remainder=True)\n",
    "        win_dataset = win_dataset.flat_map(lambda window: window.batch(window_size+1))\n",
    "        split_dataset = win_dataset.map(lambda window: (window[:-1],window[-1]))\n",
    "\n",
    "        labels = np.zeros((N-window_size+1,1))\n",
    "        newcol = np.zeros((N-window_size+1,window_size))\n",
    "        for j,win in enumerate(split_dataset):\n",
    "            newcol[j,:] = win[0].numpy()\n",
    "            labels[j,:] = win[1].numpy()\n",
    "\n",
    "        ds_labels[:,:,i] = labels\n",
    "        ds_windows[:,:,i] = newcol\n",
    "\n",
    "    return ds_windows, ds_labels\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(testcol)\n",
    "# win_dataset = dataset.window(window_size+1,1,drop_remainder=True)\n",
    "# win_dataset = win_dataset.flat_map(lambda window: window.batch(window_size+1))\n",
    "# split_dataset = win_dataset.map(lambda window: (window[:-1],window[-1]))\n",
    "\n",
    "# newcol = np.zeros((3829,12))\n",
    "# labels = []\n",
    "# for i,win in enumerate(win_dataset):\n",
    "#     newcol[i,:] = list(win.as_numpy_iterator())\n",
    "\n",
    "# Don't need to shuffle \n",
    "\n",
    "# def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "#   dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "#   dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "#   dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "#   dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
    "#   dataset = dataset.batch(batch_size).prefetch(1)\n",
    "#   return dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of windowed data: 3829,12,32; shape of labels: 3829,1,32\n",
    "# Shape of features: 3829,10,32\n",
    "# N = no. of examples, M = no. of channels, \n",
    "# Shape of windowed data: N-window_size+1,window_size,M\n",
    "# Shape of labels: N-window_size+1,1,M\n",
    "# Shape of features: N-window_size=1,10,M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(3829, 12, 32)\n(3829, 1, 32)\n(3829, 12, 32)\n(3829, 1, 32)\n"
    }
   ],
   "source": [
    "think_xtrainset,think_ytrainset = windowed_dataset(think_df,12)\n",
    "count_xtrainset,count_ytrainset = windowed_dataset(count_df,12)\n",
    "\n",
    "print(think_xtrainset.shape)\n",
    "print(think_ytrainset.shape)\n",
    "print(count_xtrainset.shape)\n",
    "print(count_ytrainset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"F:\\EEG-data\\\\think-count\\\\raw\\windows/think_xtrain\",think_xtrainset)\n",
    "np.save(\"F:\\EEG-data\\\\think-count\\\\raw\\windows/think_ytrain\",think_ytrainset)\n",
    "np.save(\"F:\\EEG-data\\\\think-count\\\\raw\\windows/count_xtrain\",count_xtrainset)\n",
    "np.save(\"F:\\EEG-data\\\\think-count\\\\raw\\windows/count_ytrain\",count_ytrainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_18\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_66 (Dense)             (None, 10)                130       \n_________________________________________________________________\ndense_67 (Dense)             (None, 10)                110       \n_________________________________________________________________\ndense_65 (Dense)             (None, 1)                 11        \n=================================================================\nTotal params: 251\nTrainable params: 251\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "lastDense = tf.keras.layers.Dense(1, activation=\"relu\")\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape = [window_size], activation=\"relu\"), \n",
    "    tf.keras.layers.Dense(10, activation=\"relu\"), \n",
    "    lastDense\n",
    "])\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer='adam',\n",
    "              metrics=[\"mae\"])\n",
    "# model.compile(loss='mae',optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(10)\n",
    "N,_,M = think_xtrainset.shape\n",
    "ds_weights = np.zeros((N,10,M))\n",
    "\n",
    "for i in range(N): #N\n",
    "    for j in range(M):#M\n",
    "        train_set = tf.data.Dataset.from_tensor_slices((think_xtrainset[i,:,j].reshape(1,12), think_ytrainset[i,:,j].reshape(1,1)))        \n",
    "        train_set = train_set.batch(1)\n",
    "        model.fit(train_set,epochs=10,verbose=0)\n",
    "        weights = (lastDense.get_weights()[0]).flatten()\n",
    "        ds_weights[i,:,j] = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-0.12320234  0.07362305 -0.0142203   0.7342698   0.03959388 -0.00679281\n  0.38223428 -0.53269953 -0.40846771  0.71088701]\n"
    }
   ],
   "source": [
    "print(ds_weights[10,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.]\n"
    }
   ],
   "source": [
    "# N,_,M = think_xtrainset.shape\n",
    "train_set = tf.data.Dataset.from_tensor_slices((think_xtrainset[1,:,1].reshape(1,12), think_ytrainset[1,:,1].reshape(1,1)))\n",
    "train_set = train_set.batch(1).repeat()\n",
    "history = model.fit(train_set,epochs=10,verbose=0,steps_per_epoch=100)\n",
    "x = think_xtrainset[1,:,1].reshape(1,12)\n",
    "y = think_ytrainset[1,:,1].reshape(1,1)\n",
    "ypred = model.predict(x).flatten()\n",
    "print(ypred)\n",
    "# time = np.linspace(1,10,13)\n",
    "# plt.plot(time[:-1],x.flatten())\n",
    "# plt.plot(time[-1],y,'x')\n",
    "# plt.plot(time[-1],ypred,'o')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bit4f00f251aa71407b905d36ad95b25cdd",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}