{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-level DWT method for feature extraction \n",
    "\n",
    "Think-count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pickle as pkl\n",
    "import itertools \n",
    "import glob\n",
    "from sklearn import svm \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from spectrum import data_cosine, dpss, pmtm\n",
    "import pywt \n",
    "\n",
    "\n",
    "# %matplotlib inline \n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing raw data files \n",
    "\n",
    "#.csv path\n",
    "csvpath = \"C:/Users/Wu Di/Documents/EEG-analysis/200108-Readings-csv/thinking-counting-switching.csv\"\n",
    "\n",
    "#Read .csv files\n",
    "cols_to_use = list(range(4, 36))\n",
    "\n",
    "#Raw dataframes - each channel is a column\n",
    "raw_df = pd.read_csv(csvpath, header=None, usecols=cols_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bandpass (BP) filter helper functions\n",
    "\n",
    "#Creates butterworth BP filter\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5*fs  # Nyquist frequency, which is half of fs\n",
    "    low = lowcut/nyq  # Digital butterworth filter cutoffs must be normalized to Nyquist frequency\n",
    "    high = highcut/nyq\n",
    "    b, a = signal.butter(order, [low, high], btype=\"bandpass\")\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass(cutFreq,fs,order=5):\n",
    "    nyq = 0.5*fs\n",
    "    cutFreq = cutFreq/nyq\n",
    "    b,a = signal.butter(order,cutFreq,btype=\"lowpass\")\n",
    "    return b,a \n",
    "\n",
    "def butter_highpass(cutFreq,fs,order=5):\n",
    "    nyq = 0.5*fs\n",
    "    cutFreq = cutFreq/nyq\n",
    "    b,a = signal.butter(order,cutFreq,btype=\"highpass\")\n",
    "    return b,a \n",
    "\n",
    "#Applies butterworth BP filter\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "#     filtered_data = signal.lfilter(b, a, data)\n",
    "    filtered_data = signal.filtfilt(b,a,data)\n",
    "    return filtered_data\n",
    "\n",
    "#Applies butterworth lowpass filter\n",
    "def butter_lowpass_filter(data, cutFreq, fs, order=5):\n",
    "    b, a = butter_lowpass(cutFreq,fs,order=5)\n",
    "    filtered_data = signal.filtfilt(b,a,data)\n",
    "    return filtered_data\n",
    "\n",
    "#Applies butterworth lowpass filter\n",
    "def butter_highpass_filter(data, cutFreq, fs, order=5):\n",
    "    b, a = butter_highpass(cutFreq,fs,order=5)\n",
    "    filtered_data = signal.filtfilt(b,a,data)\n",
    "    return filtered_data\n",
    "\n",
    "#Applies butterworth BP filter to Pandas dataframe \n",
    "def bp_filter_df(df, lowcut, highcut, fs, order):\n",
    "    rows, cols = df.shape  # Get no. of rows and cols in df\n",
    "    new_index = range(1, rows+1)\n",
    "    new_cols = range(1, cols+1)\n",
    "    # Create new df with same no. of rows and cols\n",
    "    new_df = pd.DataFrame(index=new_index, columns=new_cols)\n",
    "    # new_df = new_df.fillna(0) #Fill in 0 for all values\n",
    "    for i in range(cols):  # Apply bp filter each column (channel) and saves in new_df\n",
    "        filt_col = butter_bandpass_filter(\n",
    "            df.iloc[:, i].values, lowcut, highcut, fs, order)\n",
    "        new_df[i+1] = filt_col\n",
    "    return new_df\n",
    "\n",
    "#Applies butterworth lowpass filter to Pandas dataframe \n",
    "def lp_filter_df(df, cutFreq, fs, order):\n",
    "    rows, cols = df.shape  # Get no. of rows and cols in df\n",
    "    new_index = range(1, rows+1)\n",
    "    new_cols = range(1, cols+1)\n",
    "    # Create new df with same no. of rows and cols\n",
    "    new_df = pd.DataFrame(index=new_index, columns=new_cols)\n",
    "    # new_df = new_df.fillna(0) #Fill in 0 for all values\n",
    "    for i in range(cols):  # Apply bp filter each column (channel) and saves in new_df\n",
    "        filt_col = butter_lowpass_filter(\n",
    "            df.iloc[:, i].values, cutFreq, fs, order)\n",
    "        new_df[i+1] = filt_col\n",
    "    return new_df\n",
    "\n",
    "#Applies butterworth highpass filter to Pandas dataframe \n",
    "def hp_filter_df(df, cutFreq, fs, order):\n",
    "    rows, cols = df.shape  # Get no. of rows and cols in df\n",
    "    new_index = range(1, rows+1)\n",
    "    new_cols = range(1, cols+1)\n",
    "    # Create new df with same no. of rows and cols\n",
    "    new_df = pd.DataFrame(index=new_index, columns=new_cols)\n",
    "    # new_df = new_df.fillna(0) #Fill in 0 for all values\n",
    "    for i in range(cols):  # Apply bp filter each column (channel) and saves in new_df\n",
    "        filt_col = butter_highpass_filter(\n",
    "            df.iloc[:, i].values, cutFreq, fs, order)\n",
    "        new_df[i+1] = filt_col\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(3840, 32)\n(3840, 32)\n"
    }
   ],
   "source": [
    "#Apply BP filtering to raw dataframes\n",
    "def filt_freq_bands(df,fs):\n",
    "    delta = lp_filter_df(df, 4.5, fs, 10) \n",
    "    theta = bp_filter_df(df, 3.5, 8.5, fs, 6)\n",
    "    alpha = bp_filter_df(df, 7.5, 12.5, fs, 8)\n",
    "    beta = bp_filter_df(df, 11.5, 30.5, fs, 16)\n",
    "    gamma = hp_filter_df(df, 29.5, fs, 50)\n",
    "    return [delta, theta, alpha, beta, gamma]\n",
    "\n",
    "fs = 128\n",
    "# order = 6\n",
    "\n",
    "all_bands_list = filt_freq_bands(raw_df,fs)\n",
    "\n",
    "#Split into thinking and counting data frames\n",
    "think_index_list = []\n",
    "count_index_list = []\n",
    "\n",
    "for i in range(6):\n",
    "    if i%2==0:\n",
    "        think_index_list+=(list(range(1280*i,1280*(i+1))))\n",
    "    else:\n",
    "        count_index_list+=(list(range(1280*i,1280*(i+1))))\n",
    "\n",
    "think_bands_list = []\n",
    "count_bands_list = []\n",
    "\n",
    "for i in range(len(all_bands_list)):\n",
    "    df = all_bands_list[i].iloc[0:1280*6]\n",
    "    df_list = np.vsplit(df,6)\n",
    "    think_df = pd.DataFrame(np.vstack((df_list[0],df_list[2],df_list[4])))\n",
    "    think_bands_list.append(think_df)\n",
    "    count_df = pd.DataFrame(np.vstack((df_list[1],df_list[3],df_list[5])))\n",
    "    count_bands_list.append(count_df)\n",
    "\n",
    "print(think_bands_list[0].shape)\n",
    "print(count_bands_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only the alpha and theta bands\n",
    "#\"TA\" meaning theta and alpha\n",
    "think_TA_list = think_bands_list[1:3]\n",
    "count_TA_list = count_bands_list[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2\n2\n"
    }
   ],
   "source": [
    "print(len(think_TA_list))\n",
    "print(len(count_TA_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits a single dataframe into list of equally sized arrays\n",
    "#Each element in list is nx32 array, where n= sample length \n",
    "def split_df(df,fs,sample_t,check=False):\n",
    "    rows,_ = df.shape #get no. of rows\n",
    "    sample_len = int(sample_t*fs) #find no. of recorded samples required for each sample time length\n",
    "    Ns = int(rows/sample_len) #find total no. of samples\n",
    "    df_cut = df.iloc[:Ns*sample_len] #truncate dataframe to exact multiple of sample length\n",
    "    # print(Ns*sample_len)\n",
    "    # print(Ns)\n",
    "    df_split_list = np.vsplit(df_cut,Ns) #split dataframe row-wise, returns a list\n",
    "    \n",
    "    if check:\n",
    "        print(\"Total no. of recorded samples: \"+str(rows))\n",
    "        print(\"Sample length: \"+str(sample_len))\n",
    "        print(\"Total no. of samples: \"+str(Ns))\n",
    "        print(\"Length of df_split_list: \"+str(len(df_split_list)))\n",
    "        \n",
    "        if all(isinstance(x.shape,tuple) for x in df_split_list):\n",
    "            print(\"Shape of each element in df_split_list: \"+str(df_split_list[0].shape))\n",
    "        else:\n",
    "            print(\"Shapes are wrong.\")\n",
    "            for x in df_split_list:\n",
    "                print(x.shape)\n",
    "    return df_split_list,Ns\n",
    "\n",
    "#Apply split_df() function to list of dataframes, reshape dataframe such that each element is an array \n",
    "#for the appropriate sample time length \n",
    "def split_bands_list(bands_list,fs,sample_t,check=False,checkSD=False):\n",
    "    df_list_rFE = [0]*len(bands_list) #dataframes list ready for feature extraction \n",
    "    for df_no in range(len(bands_list)):\n",
    "        df_split_list,Ns = split_df(bands_list[df_no],fs,sample_t,check=checkSD)\n",
    "        list_of_series = [0]*Ns\n",
    "        for i in range(len(df_split_list)):\n",
    "            #New dataframe will have shape Nsx32, each element is a 1xsample_len array \n",
    "            new_row = [0]*32 \n",
    "            #Each df_split_list[i] is a dataframe\n",
    "            for j in range(len(df_split_list[i].columns)):\n",
    "                new_row[j] = df_split_list[i].iloc[:,j].values \n",
    "            list_of_series[i] = new_row\n",
    "        df_list_rFE[df_no] = pd.DataFrame(list_of_series)\n",
    "    if check:\n",
    "        print(\"Length of bands_list: \"+str(len(bands_list)))\n",
    "        print(\"Length of df_list_rFE: \"+str(len(df_list_rFE)))\n",
    "        if (all(isinstance(x.shape,tuple) for x in df_list_rFE)) and (Ns==len(df_list_rFE[0].index)):\n",
    "            print(\"Shape of each dataframe in df_list_rFE: \"+str(df_list_rFE[0].shape))\n",
    "    return df_list_rFE\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split filtered dataframes into samples\n",
    "fs = 128 #sampling freq\n",
    "sample_t_list = [0.05,0.1] #sample time lengths in seconds\n",
    "\n",
    "#Function to apply split_bands_list function once for each sample time\n",
    "def apply_multipleSplits(bands_list,fs,sample_t_list,checks=[True,False]):\n",
    "    #Output is a list of bands_split lists\n",
    "    bands_splits_lists = [0]*len(sample_t_list)\n",
    "    for i in range(len(sample_t_list)):\n",
    "        bands_splits_lists[i] = split_bands_list(bands_list,fs,sample_t_list[i],check=checks[0],checkSD=checks[1])\n",
    "    return bands_splits_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Length of bands_list: 5\nLength of df_list_rFE: 5\nShape of each dataframe in df_list_rFE: (640, 32)\nLength of bands_list: 5\nLength of df_list_rFE: 5\nShape of each dataframe in df_list_rFE: (320, 32)\n"
    }
   ],
   "source": [
    "T_splits_list = apply_multipleSplits(think_bands_list,fs,sample_t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Length of bands_list: 5\nLength of df_list_rFE: 5\nShape of each dataframe in df_list_rFE: (640, 32)\nLength of bands_list: 5\nLength of df_list_rFE: 5\nShape of each dataframe in df_list_rFE: (320, 32)\n"
    }
   ],
   "source": [
    "C_splits_list = apply_multipleSplits(count_bands_list,fs,sample_t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Length of bands_list: 2\nLength of df_list_rFE: 2\nShape of each dataframe in df_list_rFE: (640, 32)\nLength of bands_list: 2\nLength of df_list_rFE: 2\nShape of each dataframe in df_list_rFE: (320, 32)\n"
    }
   ],
   "source": [
    "T_TA_splits_list = apply_multipleSplits(think_TA_list,fs,sample_t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Length of bands_list: 2\nLength of df_list_rFE: 2\nShape of each dataframe in df_list_rFE: (640, 32)\nLength of bands_list: 2\nLength of df_list_rFE: 2\nShape of each dataframe in df_list_rFE: (320, 32)\n"
    }
   ],
   "source": [
    "C_TA_splits_list = apply_multipleSplits(count_TA_list,fs,sample_t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save these dataframes for future use \n",
    "## Splits_list contains:\n",
    "## N lists, for N = len(sample_t_list)\n",
    "## In each list, it has 5 dataframes for 5 frequency bands - delta,theta,alpha,beta,gamma\n",
    "def save_df_fromSplitsList(splits_list,savedir,filename,sample_t_list,freqBand_list):\n",
    "    for (i,listNo) in enumerate(splits_list):\n",
    "        sample_t_string = str(sample_t_list[i])+\"s/\"\n",
    "        for (j,freqBand) in enumerate(listNo):\n",
    "            freqBand_string = str(freqBand_list[j])+\"/\"\n",
    "            mod_savedir = savedir+sample_t_string+freqBand_string\n",
    "            freqBand.to_pickle(mod_savedir+filename+\".pkl\")\n",
    "            freqBand.to_csv(mod_savedir+filename+\".csv\")\n",
    "\n",
    "savedir = \"F:\\EEG-data\\\\think-count\\\\filtered/\"\n",
    "freqBand_list = [\"delta\",\"theta\",\"alpha\",\"beta\",\"gamma\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_fromSplitsList(T_splits_list,savedir,\"think\",sample_t_list,freqBand_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_fromSplitsList(C_splits_list,savedir,\"count\",sample_t_list,freqBand_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2\n2\n"
    }
   ],
   "source": [
    "#Use theta and alpha bands only\n",
    "print(len(T_TA_splits_list))\n",
    "print(len(C_TA_splits_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dwt decomposition, 4 levels,  \"db4\" wavelet -> yields 5 lists of coefficients (cA4,cD4,cD3,cD2,cD1)\n",
    "\n",
    "# [[0.05:theta,alpha],[0.1:theta,alpha]]\n",
    "\n",
    "# a = T_TA_splits_list[0][0].iloc[0,0]\n",
    "# coeffs = pywt.wavedec(a,'db4',level=4)\n",
    "# flat = [item for sublist in coeffs for item in sublist]\n",
    "# flat = lambda coeff: np.array([item for sublist in coeff for item in sublist])\n",
    "# flat(coeffs)\n",
    "\n",
    "def dwt_bands_list(bands_list):\n",
    "    #Applies dwt to get coefficients, concatenate the lists of coefficients as 1 list\n",
    "    #Use for each list containing frequency subbands \n",
    "    dwt_df_list = [0]*len(bands_list)\n",
    "    for (i,df) in enumerate(bands_list):\n",
    "        dwt_df_list[i] = df.applymap(lambda x: pywt.wavedec(x,'db4',level=4))\n",
    "        dwt_df_list[i] = dwt_df_list[i].applymap(lambda x: np.array([item for sublist in x for item in sublist]))\n",
    "    return dwt_df_list\n",
    "\n",
    "def mul_dwt_bands_list(mul_bands_list):\n",
    "    #Applies dwt_bands_list for list containing multiple sample_t \n",
    "    mul_dwt_df_list = [0]*len(mul_bands_list)\n",
    "    for (i,df_list) in enumerate(mul_bands_list):\n",
    "        mul_dwt_df_list[i] = dwt_bands_list(df_list)\n",
    "    return mul_dwt_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_mul_dwt_df_list =  mul_dwt_bands_list(T_TA_splits_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_mul_dwt_df_list =  mul_dwt_bands_list(C_TA_splits_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(640, 32)\n(640, 32)\n(320, 32)\n(320, 32)\n(640, 32)\n(640, 32)\n(320, 32)\n(320, 32)\n"
    }
   ],
   "source": [
    "def check_combos_shapes(combos_df_list):\n",
    "    for i in range(len(combos_df_list)):\n",
    "        for j in range(len(combos_df_list[i])):\n",
    "            print(combos_df_list[i][j].shape)\n",
    "\n",
    "check_combos_shapes(T_mul_dwt_df_list)\n",
    "check_combos_shapes(C_mul_dwt_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2\nT00 shape:(640, 960)\nC00 shape:(640, 960)\nT01 shape:(640, 960)\nC01 shape:(640, 960)\nT10 shape:(320, 1216)\nC10 shape:(320, 1216)\nT11 shape:(320, 1216)\nC11 shape:(320, 1216)\n"
    }
   ],
   "source": [
    "#Expand all lists in cells to their own variables\n",
    "\n",
    "#Expands for a single feature_df_list\n",
    "def expand_PSD_df_list(PSD_df_list):\n",
    "    e_PSD_df_list = [0]*len(PSD_df_list)\n",
    "    for df_no in range(len(PSD_df_list)):\n",
    "        #e_PSD_df_cols_list will be used to create new dataframe\n",
    "        no_PSD = len(PSD_df_list[0].iloc[0,0])\n",
    "        e_PSD_df_cols_list = [0]*32\n",
    "        \n",
    "        for channel in range(len(PSD_df_list[df_no].columns)):\n",
    "            #Expand each column into its own dataframe\n",
    "            new_col = PSD_df_list[df_no][channel].apply(pd.Series)\n",
    "            #Rename every variable in the new column\n",
    "            new_col = new_col.rename(columns = lambda x: \"Ch\"+str(channel+1)+'_'+str(np.linspace(0,64,no_PSD)[x]))\n",
    "            #Add new_col to cols_list\n",
    "            e_PSD_df_cols_list[channel] = new_col\n",
    "        \n",
    "        #Create new dataframe\n",
    "        e_PSD_df = pd.concat(e_PSD_df_cols_list, axis=1)\n",
    "        \n",
    "        #Add to list\n",
    "        e_PSD_df_list[df_no] = e_PSD_df\n",
    "    return e_PSD_df_list \n",
    "\n",
    "#Expands for a list of feature_df_list\n",
    "def mul_expand_PSD_df_list(sampLen_PSD_df_list):\n",
    "    sampLen_e_PSD_df_list = [0]*len(sampLen_PSD_df_list)\n",
    "    for i in range(len(sampLen_PSD_df_list)):\n",
    "        sampLen_e_PSD_df_list[i] = expand_PSD_df_list(sampLen_PSD_df_list[i])\n",
    "    return sampLen_e_PSD_df_list\n",
    "\n",
    "\n",
    "T_e_mul_dwt_df_list = mul_expand_PSD_df_list(T_mul_dwt_df_list)\n",
    "C_e_mul_dwt_df_list = mul_expand_PSD_df_list(C_mul_dwt_df_list)\n",
    "\n",
    "if len(T_e_mul_dwt_df_list) == len(C_e_mul_dwt_df_list):\n",
    "    print(len(T_e_mul_dwt_df_list))\n",
    "    for i in range(len(T_e_mul_dwt_df_list)):\n",
    "        for j in range(len(T_e_mul_dwt_df_list[i])):\n",
    "            print(\"T\"+str(i)+str(j)+\" shape:\"+str(T_e_mul_dwt_df_list[i][j].shape))\n",
    "            print(\"C\"+str(i)+str(j)+\" shape:\"+str(C_e_mul_dwt_df_list[i][j].shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create datasets - both theta and alpha bands will be used\n",
    "\n",
    "#Apply to a single expanded PSD_df_list\n",
    "def get_1F_combos_df_list(e_PSD_df_list): \n",
    "    #single feature\n",
    "    combos = [(0,1)] #CHANGE THIS WHEN NUMBER OF COMBINATIONS CHANGE\n",
    "    combos_df_list = [0]*len(combos)\n",
    "    for i in range(len(combos)):\n",
    "        concat_list = [e_PSD_df_list[x] for x in combos[i]]\n",
    "        combos_df_list[i] = pd.concat(concat_list,axis=1)\n",
    "    return combos_df_list\n",
    "\n",
    "#Apply to a list of expanded PSD_df_lists\n",
    "def mul_get_1F_combos_df_list(sampLen_e_PSD_df_list):\n",
    "    sampLen_1F_combos_df_list = [0]*len(sampLen_e_PSD_df_list) \n",
    "    for i in range(len(sampLen_e_PSD_df_list)):\n",
    "        sampLen_1F_combos_df_list[i] = get_1F_combos_df_list(sampLen_e_PSD_df_list[i])\n",
    "    return sampLen_1F_combos_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of dataframes with only dwt coefficient features\n",
    "T_dwt_combos_df_list = mul_get_1F_combos_df_list(T_e_mul_dwt_df_list)\n",
    "C_dwt_combos_df_list = mul_get_1F_combos_df_list(C_e_mul_dwt_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature scaling all dataframes \n",
    "\n",
    "#Applies feature scaling to one combos_df_list\n",
    "def featureScaling_df(combos_df_list):\n",
    "    sc = StandardScaler()\n",
    "    scaled_combos_df_list = [0]*len(combos_df_list)\n",
    "    for i in range(len(combos_df_list)):\n",
    "        df = combos_df_list[i]\n",
    "        cols = df.columns\n",
    "        scaled_combos_df_list[i] = pd.DataFrame(sc.fit_transform(df),columns=cols)\n",
    "\n",
    "    return scaled_combos_df_list\n",
    "\n",
    "# Applies feature scaling to list of combo_df_lists\n",
    "def mul_featureScaling_df(sampLen_combos_df_list):\n",
    "    sampLen_scaled_combos_df_list = [0]*len(sampLen_combos_df_list)\n",
    "    for i in range(len(sampLen_combos_df_list)):\n",
    "        sampLen_scaled_combos_df_list[i] = featureScaling_df(sampLen_combos_df_list[i])\n",
    "    return sampLen_scaled_combos_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_T_dwt_combos_df_list = mul_featureScaling_df(T_dwt_combos_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_C_dwt_combos_df_list = mul_featureScaling_df(C_dwt_combos_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(640, 1920)\n(320, 2432)\n(640, 1920)\n(320, 2432)\n"
    }
   ],
   "source": [
    "check_combos_shapes(sc_T_dwt_combos_df_list)\n",
    "check_combos_shapes(sc_C_dwt_combos_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1\n1\n"
    }
   ],
   "source": [
    "print(len(sc_T_dwt_combos_df_list[0]))\n",
    "print(len(sc_C_dwt_combos_df_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten out list of lists since each sublist only has 1 element\n",
    "sc_T_dwt_combos_df_list_f = [item for sublist in sc_T_dwt_combos_df_list for item in sublist]\n",
    "sc_C_dwt_combos_df_list_f = [item for sublist in sc_C_dwt_combos_df_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(640, 1920)\n(320, 2432)\n(640, 1920)\n(320, 2432)\n"
    }
   ],
   "source": [
    "def check_shapes(combos_df_list_f):\n",
    "    for i in range(len(combos_df_list_f)):\n",
    "        print(combos_df_list_f[i].shape)\n",
    "\n",
    "check_shapes(sc_T_dwt_combos_df_list_f)\n",
    "check_shapes(sc_C_dwt_combos_df_list_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(640, 1921)\n(320, 2433)\n(640, 1921)\n(320, 2433)\n"
    }
   ],
   "source": [
    "#Append action type columns to all dataframes \n",
    "\n",
    "#Add action column for a list of dataframes\n",
    "def add_action_col(df_list,action_type):\n",
    "    new_list = [0]*len(df_list)\n",
    "    for i in range(len(df_list)):\n",
    "        new_df = df_list[i][:]\n",
    "        new_df['Action'] = pd.Series(action_type,index=df_list[i].index) #add new column\n",
    "        new_list[i] = new_df\n",
    "    return new_list\n",
    "\n",
    "sc_T_dwt_combosA_df_list_f = add_action_col(sc_T_dwt_combos_df_list_f,'T')\n",
    "sc_C_dwt_combosA_df_list_f = add_action_col(sc_C_dwt_combos_df_list_f,'C')\n",
    "\n",
    "check_shapes(sc_T_dwt_combosA_df_list_f)\n",
    "check_shapes(sc_C_dwt_combosA_df_list_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine different class types to form full datasets\n",
    "def concatSave_df_list(T_df_list,C_df_list,sampLenStrings,filename,savedir,save=False):\n",
    "    if len(C_df_list) == len(T_df_list):\n",
    "        for i in range(len(C_df_list)):\n",
    "            new_df = pd.concat([T_df_list[i],C_df_list[i]],axis=0)\n",
    "            if save:\n",
    "                savepath = savedir+filename+'_'+ sampLenStrings[i]+'.pkl'\n",
    "                #Save to external HDD as pkl files \n",
    "                new_df.to_pickle(savepath)\n",
    "            if i == 0:\n",
    "                csvpath = savedir+filename+'_'+ sampLenStrings[i]+'.csv'\n",
    "                new_df.to_csv(csvpath)\n",
    "    else:\n",
    "        print(\"Lists are of unequal lengths.\")\n",
    "\n",
    "sampLenStrings = [\"0.05s\",\"0.1s\"]\n",
    "dwt_savedir = \"F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/\"\n",
    "dwt_filename = \"dwt_df\"\n",
    "\n",
    "#DWT\n",
    "concatSave_df_list(sc_T_dwt_combosA_df_list_f,sc_C_dwt_combosA_df_list_f,sampLenStrings,dwt_filename,dwt_savedir,save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM, Cross Validation, Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories to load feature scaled datasets\n",
    "loaddir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/'\n",
    "# Directories to save results\n",
    "savedir = \"F:\\EEG-data\\\\think-count\\dwt\\PCA_CV_results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for SVM \n",
    "## Applies PCA only training set to retain 99% variance\n",
    "## 80% training set, 20% test set\n",
    "## 5 fold cross validation\n",
    "def apply_PCA_CV_SVM(loaddir,savedir):\n",
    "    files = glob.glob(loaddir+'*.pkl')\n",
    "    sss = StratifiedShuffleSplit(n_splits=5,test_size=0.2,random_state=0)\n",
    "    testdir = savedir+'testResults/'\n",
    "    bestF1 = {'F1 Score':0,'dataset':'','params':''}\n",
    "    test_results = {'F1 Score':[],'dataset':[],'params':[]}\n",
    "    #Create PCA instances\n",
    "    pca99 = PCA(n_components=0.99, svd_solver='full')\n",
    "    #Instantiate SVM gridsearch\n",
    "    C_range = [1,3,10,30,100,300,1000]\n",
    "    param_grid= [\n",
    "                # {'C': C_range, 'kernel': ['linear']},\n",
    "                {'C': C_range, 'gamma': [0.001, 0.0001, 'auto', 'scale'], 'kernel': ['rbf']},\n",
    "                ]\n",
    "    scoring = {'accuracy','f1_macro'}\n",
    "    clf = svm.SVC()\n",
    "    grid = GridSearchCV(clf,param_grid=param_grid,scoring=scoring,cv=5,refit='f1_macro')\n",
    "    #Apply PCA and Gridsearch\n",
    "    for file in files:\n",
    "        #Create filenames for saving\n",
    "        cv99_filename = file.split(\"\\\\\")[-1].rstrip('.pkl') +'_PCA99_CV_results.pkl'\n",
    "        df = pd.read_pickle(file)\n",
    "        X = df.iloc[:,:-1].values\n",
    "        y = df.iloc[:,-1].values\n",
    "        #Split into training and test sets\n",
    "        for train_index, test_index in sss.split(X,y):\n",
    "            X_train, X_test = X[train_index],X[test_index]\n",
    "            y_train, y_test = y[train_index],y[test_index]\n",
    "        #Fit transform on training data\n",
    "        x_99_train = pca99.fit_transform(X_train)\n",
    "        #Fit transform on test data\n",
    "        x_99_test = pca99.transform(X_test)\n",
    "        #Gridsearch\n",
    "        grid99 = grid.fit(x_99_train,y_train)\n",
    "        #Get number of principal components\n",
    "        _,cols99 = x_99_train.shape\n",
    "        #Get results\n",
    "        results_df99 = pd.DataFrame(grid99.cv_results_)\n",
    "        #Append number of principal components\n",
    "        results_df99['No. of PC'] = [cols99]*len(results_df99.index) \n",
    "        #Save results\n",
    "        results_df99.to_pickle(savedir+cv99_filename)\n",
    "        #Export as csv \n",
    "        results_df99.to_csv(savedir+cv99_filename+'.csv')\n",
    "        #Fit SVM on best parameters \n",
    "        best_params99 = results_df99.loc[results_df99['rank_test_f1_macro'].idxmin()]['params']\n",
    "        \n",
    "        clf99 = svm.SVC(**best_params99)\n",
    "        clf99.fit(x_99_train,y_train)\n",
    "        y_99_pred = clf99.predict(x_99_test)\n",
    "\n",
    "        y_pred_list = [y_99_pred]\n",
    "        x_test_list = [x_99_test]\n",
    "        clf_list = [clf99]\n",
    "        best_params_list =[best_params99]\n",
    "\n",
    "        #Generate and save classification report, macro-avg f1 score, confusion matrix\n",
    "        \n",
    "        target_names = ['T','C']\n",
    "        names_list = ['99']\n",
    "        for i in range(len(y_pred_list)):\n",
    "            df_name = file.split(\"\\\\\")[-1].rstrip('.pkl')+'_'+names_list[i]\n",
    "            report_name = file.split(\"\\\\\")[-1].rstrip('.pkl') + '_'+names_list[i]+'_PCA_CV_SVM_clf_report'\n",
    "            cm_name = file.split(\"\\\\\")[-1].rstrip('.pkl') + '_'+names_list[i]+'_PCA_CV_SVM_ConfusionMatrix'\n",
    "            ##Classification report \n",
    "            report = classification_report(y_test, y_pred_list[i], target_names=target_names, output_dict=True)\n",
    "            report_df = pd.DataFrame(report).transpose()\n",
    "            report_df.to_pickle(testdir+report_name+'.pkl')\n",
    "            ##Macro-avg f1 score \n",
    "            f1Score = report['macro avg']['f1-score']\n",
    "            test_results['F1 Score'].append(f1Score)\n",
    "            test_results['dataset'].append(df_name)\n",
    "            test_results['params'].append(best_params_list[i])\n",
    "            test_results_df = pd.DataFrame(test_results)\n",
    "            if bestF1['F1 Score']<f1Score:\n",
    "                bestF1['F1 Score'] = f1Score\n",
    "                bestF1['dataset'] = df_name\n",
    "                bestF1['params'] = str(best_params_list[i])\n",
    "            ##Confusion matrix\n",
    "            fig,ax = plt.subplots()\n",
    "            ax.set_title(df_name+' CM')\n",
    "            plot_confusion_matrix(clf_list[i],x_test_list[i],y_test,labels=target_names,ax=ax,normalize='true')\n",
    "            plt.savefig(testdir+df_name+'.png')\n",
    "            plt.close()\n",
    "\n",
    "    bestF1_df = pd.DataFrame(bestF1,index=[0],columns=['F1 Score','dataset','params'])\n",
    "    bestF1_df.to_csv(testdir+bestF1['dataset']+'.csv')\n",
    "    test_results_df.to_csv(testdir+'test_results.csv')\n",
    "    test_results_df.to_csv(testdir+'test_results.pkl')\n",
    "        \n",
    "\n",
    "\n",
    "apply_PCA_CV_SVM(loaddir,savedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding stastistical features from dwt coefficients \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bit4f00f251aa71407b905d36ad95b25cdd",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}