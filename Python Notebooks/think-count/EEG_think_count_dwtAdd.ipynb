{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-level DWT method for feature extraction \n",
    "\n",
    "Think-count\n",
    "\n",
    "dwt coefficients, RMS for each set of coefficients, variance for each set of coefficients, ratio between bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pickle as pkl\n",
    "import itertools \n",
    "import glob\n",
    "from sklearn import svm \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from spectrum import data_cosine, dpss, pmtm\n",
    "import pywt \n",
    "\n",
    "\n",
    "# %matplotlib inline \n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories to load filtered datasets\n",
    "theta5 = \"F:\\EEG-data\\\\think-count\\\\filtered\\\\0.05s\\\\theta/\"\n",
    "alpha5 = \"F:\\EEG-data\\\\think-count\\\\filtered\\\\0.05s\\\\alpha/\"\n",
    "\n",
    "T_TA_list = [pd.read_pickle(theta5+\"think.pkl\"),pd.read_pickle(alpha5+\"think.pkl\")]\n",
    "C_TA_list = [pd.read_pickle(theta5+\"count.pkl\"),pd.read_pickle(alpha5+\"count.pkl\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dwt_bands_list(bands_list):\n",
    "    #Applies dwt to get coefficients, \n",
    "    #Use for each list containing frequency subbands \n",
    "    dwt_df_list = [0]*len(bands_list)\n",
    "    for (i,df) in enumerate(bands_list):\n",
    "        # dwt_df_list[i] = df.applymap(lambda x: pywt.wavedec(x,'db4',level=4))\n",
    "        # dwt_df_list[i] = df.applymap(lambda x: pywt.wavedec(x,'db3',level=4))\n",
    "        # dwt_df_list[i] = df.applymap(lambda x: pywt.wavedec(x,'db2',level=4))\n",
    "        # dwt_df_list[i] = df.applymap(lambda x: pywt.wavedec(x,'db1',level=4))\n",
    "        # dwt_df_list[i] = df.applymap(lambda x: pywt.wavedec(x,'db20',level=10))\n",
    "        # dwt_df_list[i] = df.applymap(lambda x: pywt.wavedec(x,'db4',level=1))\n",
    "        # dwt_df_list[i] = df.applymap(lambda x: pywt.dwt(x,'db4'))\n",
    "        # dwt_df_list[i] = df.applymap(lambda x: pywt.dwt(x,'db20'))\n",
    "        # dwt_df_list[i] = df.applymap(lambda x: pywt.dwt(x,'db1'))\n",
    "        # dwt_df_list[i] = df.applymap(lambda x: pywt.dwt(x,'sym4'))\n",
    "        # dwt_df_list[i] = df.applymap(lambda x: pywt.dwt(x,'haar'))\n",
    "        # dwt_df_list[i] = df.applymap(lambda x: pywt.dwt(x,'coif4'))\n",
    "        # dwt_df_list[i] = df.applymap(lambda x: pywt.dwt(x,'rbio4.4'))\n",
    "        # dwt_df_list[i] = df.applymap(lambda x: pywt.dwt(x,'bior4.4'))\n",
    "        # dwt_df_list[i] = df.applymap(lambda x: pywt.dwt(x,'dmey'))\n",
    "        # dwt_df_list[i] = df.applymap(lambda x: pywt.dwt(x,'bior6.8'))\n",
    "        # dwt_df_list[i] = df.applymap(lambda x: pywt.dwt(x,'bior1.1'))\n",
    "        dwt_df_list[i] = df.applymap(lambda x: pywt.wavedec(x,'bior6.8',level=10))\n",
    "\n",
    "\n",
    "    return dwt_df_list\n",
    "\n",
    "def dwt_oneArr_bands_list(dwt_bands_list):\n",
    "    #Concatenate the lists of coefficients as 1 list\n",
    "    #Use for each list containing frequency subbands \n",
    "    dwt_oneArr_df_list = [0]*len(dwt_bands_list)\n",
    "    for (i,df) in enumerate(dwt_bands_list):\n",
    "        dwt_oneArr_df_list[i] = dwt_bands_list[i].applymap(lambda x: np.array([item for sublist in x for item in sublist]))\n",
    "    return dwt_oneArr_df_list\n",
    "\n",
    "def dwt_rms_bands_list(dwt_bands_list):\n",
    "    #Get rms of each coefficient array, then concate as 1 list \n",
    "    #Use for each list containing frequency subbands \n",
    "    dwt_rms_df_list = [0]*len(dwt_bands_list)\n",
    "    for (i,df) in enumerate(dwt_bands_list):\n",
    "        dwt_rms_df_list[i] = df.applymap(lambda x: np.array([np.sqrt(np.mean(np.square(y))) for y in x]))\n",
    "    return dwt_rms_df_list\n",
    "\n",
    "def dwt_var_bands_list(dwt_bands_list):\n",
    "    #Get var of each coefficient array, then concate as 1 list \n",
    "    #Use for each list containing frequency subbands \n",
    "    dwt_var_df_list = [0]*len(dwt_bands_list)\n",
    "    for (i,df) in enumerate(dwt_bands_list):\n",
    "        dwt_var_df_list[i] = dwt_bands_list[i].applymap(lambda x: np.array([np.var(y) for y in x]))\n",
    "    return dwt_var_df_list\n",
    "\n",
    "\n",
    "#DWT\n",
    "T_dwt_list = dwt_bands_list(T_TA_list)\n",
    "C_dwt_list = dwt_bands_list(C_TA_list)\n",
    "#Single Array\n",
    "T_dwt_1_list = dwt_oneArr_bands_list(T_dwt_list)\n",
    "C_dwt_1_list = dwt_oneArr_bands_list(C_dwt_list)\n",
    "#RMS\n",
    "T_dwt_rms_list = dwt_rms_bands_list(T_dwt_list)\n",
    "C_dwt_rms_list = dwt_rms_bands_list(C_dwt_list)\n",
    "#Variance\n",
    "T_dwt_var_list = dwt_var_bands_list(T_dwt_list)\n",
    "C_dwt_var_list = dwt_var_bands_list(C_dwt_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2\n2\n168\n"
    }
   ],
   "source": [
    "print(len(T_dwt_1_list))\n",
    "print(len(T_dwt_rms_list))\n",
    "print(len(T_dwt_1_list[1].iloc[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expand all lists in cells to their own variables\n",
    "\n",
    "#Expands for a single feature_df_list\n",
    "def expand_PSD_df_list(PSD_df_list):\n",
    "    e_PSD_df_list = [0]*len(PSD_df_list)\n",
    "    for df_no in range(len(PSD_df_list)):\n",
    "        #e_PSD_df_cols_list will be used to create new dataframe\n",
    "        no_PSD = len(PSD_df_list[0].iloc[0,0])\n",
    "        e_PSD_df_cols_list = [0]*32\n",
    "        \n",
    "        for channel in range(len(PSD_df_list[df_no].columns)):\n",
    "            #Expand each column into its own dataframe\n",
    "            new_col = PSD_df_list[df_no][channel].apply(pd.Series)\n",
    "            #Rename every variable in the new column\n",
    "            new_col = new_col.rename(columns = lambda x: \"Ch\"+str(channel+1)+'_'+str(np.linspace(0,64,no_PSD)[x]))\n",
    "            #Add new_col to cols_list\n",
    "            e_PSD_df_cols_list[channel] = new_col\n",
    "        \n",
    "        #Create new dataframe\n",
    "        e_PSD_df = pd.concat(e_PSD_df_cols_list, axis=1)\n",
    "        \n",
    "        #Add to list\n",
    "        e_PSD_df_list[df_no] = e_PSD_df\n",
    "    return e_PSD_df_list \n",
    "\n",
    "#Single Array\n",
    "e_T_dwt_1_list = expand_PSD_df_list(T_dwt_1_list)\n",
    "e_C_dwt_1_list = expand_PSD_df_list(C_dwt_1_list)\n",
    "#RMS\n",
    "e_T_dwt_rms_list = expand_PSD_df_list(T_dwt_rms_list)\n",
    "e_C_dwt_rms_list = expand_PSD_df_list(C_dwt_rms_list)\n",
    "#Variance\n",
    "e_T_dwt_var_list = expand_PSD_df_list(T_dwt_var_list)\n",
    "e_C_dwt_var_list = expand_PSD_df_list(C_dwt_var_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply to one set of expanded PSD_df_list and AUC_df_list\n",
    "def get_3F_combos_df_list(feat1_df_list,feat2_df_list,feat3_df_list):\n",
    "    #Two features\n",
    "    combos= [(0,1)]\n",
    "    combos_df_list = [0]*len(combos)\n",
    "    for i in range(len(combos)):\n",
    "        feat1_list = [feat1_df_list[x] for x in combos[i]]\n",
    "        feat2_list = [feat2_df_list[x] for x in combos[i]]\n",
    "        feat3_list = [feat3_df_list[x] for x in combos[i]]\n",
    "        concat_list = feat1_list + feat2_list + feat3_list\n",
    "        combos_df_list[i] = pd.concat(concat_list,axis=1)\n",
    "    return combos_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of dataframes with both features\n",
    "T_combos_df_list = get_3F_combos_df_list(e_T_dwt_1_list,e_T_dwt_rms_list,e_T_dwt_var_list)\n",
    "C_combos_df_list = get_3F_combos_df_list(e_C_dwt_1_list,e_C_dwt_rms_list,e_C_dwt_var_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature scaling all dataframes \n",
    "\n",
    "#Applies feature scaling to one combos_df_list\n",
    "def featureScaling_df(combos_df_list):\n",
    "    sc = StandardScaler()\n",
    "    scaled_combos_df_list = [0]*len(combos_df_list)\n",
    "    for i in range(len(combos_df_list)):\n",
    "        df = combos_df_list[i]\n",
    "        cols = df.columns\n",
    "        scaled_combos_df_list[i] = pd.DataFrame(sc.fit_transform(df),columns=cols)\n",
    "\n",
    "    return scaled_combos_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_T_combos_df_list = featureScaling_df(T_combos_df_list)\n",
    "sc_C_combos_df_list = featureScaling_df(C_combos_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append action type columns to all dataframes \n",
    "\n",
    "#Add action column for a list of dataframes\n",
    "def add_action_col(df_list,action_type):\n",
    "    new_list = [0]*len(df_list)\n",
    "    for i in range(len(df_list)):\n",
    "        new_df = df_list[i][:]\n",
    "        new_df['Action'] = pd.Series(action_type,index=df_list[i].index) #add new column\n",
    "        new_list[i] = new_df\n",
    "    return new_list\n",
    "\n",
    "sc_T_combosA_df_list = add_action_col(sc_T_combos_df_list,'T')\n",
    "sc_C_combosA_df_list = add_action_col(sc_C_combos_df_list,'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine different class types to form full datasets\n",
    "def concatSave_df_list(T_df_list,C_df_list,sampLenStrings,filename,savedir,save=False):\n",
    "    if len(C_df_list) == len(T_df_list):\n",
    "        for i in range(len(C_df_list)):\n",
    "            new_df = pd.concat([T_df_list[i],C_df_list[i]],axis=0)\n",
    "            if save:\n",
    "                savepath = savedir+filename+'_'+ sampLenStrings[i]+'.pkl'\n",
    "                #Save to external HDD as pkl files \n",
    "                new_df.to_pickle(savepath)\n",
    "            if i == 0:\n",
    "                csvpath = savedir+filename+'_'+ sampLenStrings[i]+'.csv'\n",
    "                new_df.to_csv(csvpath)\n",
    "    else:\n",
    "        print(\"Lists are of unequal lengths.\")\n",
    "\n",
    "sampLenStrings = [\"0.05s\"]\n",
    "# dwt_savedir = \"F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/dwtAdd/\" #Level 4\n",
    "# dwt_savedir = \"F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/db1/\" #Level 4\n",
    "# dwt_savedir = \"F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/db2/\" #Level 4\n",
    "# dwt_savedir = \"F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/db3/\" #Level 4\n",
    "# dwt_savedir = \"F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/db20/\" #Level 4\n",
    "# dwt_savedir = \"F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/db4-lvl1/\"\n",
    "# dwt_savedir = \"F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/db20-lvl1/\"\n",
    "# dwt_savedir = \"F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/db1-lvl1/\"\n",
    "# dwt_savedir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/sym4-lvl1/'\n",
    "# dwt_savedir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/haar-lvl1/'\n",
    "# dwt_savedir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/coiflets4-lvl1/'\n",
    "# dwt_savedir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/rbiortho4.4-lvl1/'\n",
    "# dwt_savedir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/biortho4.4-lvl1/'\n",
    "# dwt_savedir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/dMeyer-lvl1/'\n",
    "# dwt_savedir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/biortho6.8-lvl1/'\n",
    "# dwt_savedir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/biortho1.1-lvl1/'\n",
    "dwt_savedir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/biortho6.8-lvl10/'\n",
    "\n",
    "\n",
    "# dwt_filename = \"dwtAdd_df\"\n",
    "# dwt_filename = \"db1_df\"\n",
    "# dwt_filename = \"db2_df\"\n",
    "# dwt_filename = \"db3_df\"\n",
    "# dwt_filename = \"db20_df\"\n",
    "# dwt_filename = \"db4-lvl1_df\"\n",
    "# dwt_filename = \"db20-lvl1_df\"\n",
    "# dwt_filename = \"db1-lvl1_df\"\n",
    "# dwt_filename = \"sym4-lvl1_df\"\n",
    "# dwt_filename = \"haar-lvl1_df\"\n",
    "# dwt_filename = \"coiflets4-lvl1_df\"\n",
    "# dwt_filename = \"rbiortho4-lvl1_df\"\n",
    "# dwt_filename = \"biortho4-lvl1_df\"\n",
    "# dwt_filename = \"dMeyer-lvl1_df\"\n",
    "# dwt_filename = \"biortho68-lvl1_df\"\n",
    "# dwt_filename = \"biortho11-lvl1_df\"\n",
    "dwt_filename = \"biortho68-lvl10_df\"\n",
    "\n",
    "\n",
    "\n",
    "#DWT\n",
    "concatSave_df_list(sc_T_combosA_df_list,sc_C_combosA_df_list,sampLenStrings,dwt_filename,dwt_savedir,save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM, Cross Validation, Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories to load feature scaled datasets\n",
    "# loaddir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/dwtAdd/'\n",
    "# loaddir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/db1/'\n",
    "# loaddir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/db2/'\n",
    "# loaddir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/db3/'\n",
    "# loaddir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/db20/'\n",
    "# loaddir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/db4-lvl1/'\n",
    "# loaddir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/db20-lvl1/'\n",
    "# loaddir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/db1-lvl1/'\n",
    "# loaddir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/sym4-lvl1/'\n",
    "# loaddir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/haar-lvl1/'\n",
    "# loaddir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/coiflets4-lvl1/'\n",
    "# loaddir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/rbiortho4.4-lvl1/'\n",
    "# loaddir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/biortho4.4-lvl1/'\n",
    "# loaddir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/dMeyer-lvl1/'\n",
    "# loaddir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/biortho6.8-lvl1/'\n",
    "# loaddir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/biortho1.1-lvl1/'\n",
    "loaddir = 'F:\\EEG-data\\\\think-count\\dwt\\\\featureScaled/biortho6.8-lvl10/'\n",
    "\n",
    "\n",
    "# Directories to save results\n",
    "# savedir = \"F:\\EEG-data\\\\think-count\\dwt\\PCA_CV_results/dwtAdd/\"\n",
    "# savedir = \"F:\\EEG-data\\\\think-count\\dwt\\PCA_CV_results/db1/\"\n",
    "# savedir = \"F:\\EEG-data\\\\think-count\\dwt\\PCA_CV_results/db2/\"\n",
    "# savedir = \"F:\\EEG-data\\\\think-count\\dwt\\PCA_CV_results/db3/\"\n",
    "# savedir = \"F:\\EEG-data\\\\think-count\\dwt\\PCA_CV_results/db20/\"\n",
    "# savedir = \"F:\\EEG-data\\\\think-count\\dwt\\PCA_CV_results/db4-lvl1/\"\n",
    "# savedir = \"F:\\EEG-data\\\\think-count\\dwt\\PCA_CV_results/db20-lvl1/\"\n",
    "# savedir = \"F:\\EEG-data\\\\think-count\\dwt\\PCA_CV_results/db1-lvl1/\"\n",
    "# savedir = 'F:\\EEG-data\\\\think-count\\dwt\\\\PCA_CV_results/sym4-lvl1/'\n",
    "# savedir = 'F:\\EEG-data\\\\think-count\\dwt\\\\PCA_CV_results/haar-lvl1/'\n",
    "# savedir = 'F:\\EEG-data\\\\think-count\\dwt\\\\PCA_CV_results/coiflets4-lvl1/'\n",
    "# savedir = 'F:\\EEG-data\\\\think-count\\dwt\\\\PCA_CV_results/rbiortho4.4-lvl1/'\n",
    "# savedir = 'F:\\EEG-data\\\\think-count\\dwt\\\\PCA_CV_results/biortho4.4-lvl1/'\n",
    "# savedir = 'F:\\EEG-data\\\\think-count\\dwt\\\\PCA_CV_results/dMeyer-lvl1/'\n",
    "# savedir = 'F:\\EEG-data\\\\think-count\\dwt\\\\PCA_CV_results/biortho6.8-lvl1/'\n",
    "# savedir = 'F:\\EEG-data\\\\think-count\\dwt\\\\PCA_CV_results/biortho1.1-lvl1/'\n",
    "savedir = 'F:\\EEG-data\\\\think-count\\dwt\\\\PCA_CV_results/biortho6.8-lvl10/'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for SVM \n",
    "## Applies PCA only training set to retain 99% variance\n",
    "## 80% training set, 20% test set\n",
    "## 5 fold cross validation\n",
    "def apply_PCA_CV_SVM(loaddir,savedir):\n",
    "    files = glob.glob(loaddir+'*.pkl')\n",
    "    sss = StratifiedShuffleSplit(n_splits=5,test_size=0.2,random_state=0)\n",
    "    testdir = savedir+'testResults/'\n",
    "    bestF1 = {'F1 Score':0,'dataset':'','params':''}\n",
    "    test_results = {'F1 Score':[],'dataset':[],'params':[]}\n",
    "    #Create PCA instances\n",
    "    pca99 = PCA(n_components=0.99, svd_solver='full')\n",
    "    #Instantiate SVM gridsearch\n",
    "    C_range = [1,3,10,30,100,300,1000]\n",
    "    param_grid= [\n",
    "                # {'C': C_range, 'kernel': ['linear']},\n",
    "                {'C': C_range, 'gamma': [0.001, 0.0001, 'auto', 'scale'], 'kernel': ['rbf']},\n",
    "                ]\n",
    "    scoring = {'accuracy','f1_macro'}\n",
    "    clf = svm.SVC()\n",
    "    grid = GridSearchCV(clf,param_grid=param_grid,scoring=scoring,cv=5,refit='f1_macro')\n",
    "    #Apply PCA and Gridsearch\n",
    "    for file in files:\n",
    "        #Create filenames for saving\n",
    "        cv99_filename = file.split(\"\\\\\")[-1].rstrip('.pkl') +'_PCA99_CV_results.pkl'\n",
    "        df = pd.read_pickle(file)\n",
    "        X = df.iloc[:,:-1].values\n",
    "        y = df.iloc[:,-1].values\n",
    "        #Split into training and test sets\n",
    "        for train_index, test_index in sss.split(X,y):\n",
    "            X_train, X_test = X[train_index],X[test_index]\n",
    "            y_train, y_test = y[train_index],y[test_index]\n",
    "        #Fit transform on training data\n",
    "        x_99_train = pca99.fit_transform(X_train)\n",
    "        #Fit transform on test data\n",
    "        x_99_test = pca99.transform(X_test)\n",
    "        #Gridsearch\n",
    "        grid99 = grid.fit(x_99_train,y_train)\n",
    "        #Get number of principal components\n",
    "        _,cols99 = x_99_train.shape\n",
    "        #Get results\n",
    "        results_df99 = pd.DataFrame(grid99.cv_results_)\n",
    "        #Append number of principal components\n",
    "        results_df99['No. of PC'] = [cols99]*len(results_df99.index) \n",
    "        #Save results\n",
    "        results_df99.to_pickle(savedir+cv99_filename)\n",
    "        #Export as csv \n",
    "        results_df99.to_csv(savedir+cv99_filename+'.csv')\n",
    "        #Fit SVM on best parameters \n",
    "        best_params99 = results_df99.loc[results_df99['rank_test_f1_macro'].idxmin()]['params']\n",
    "        \n",
    "        clf99 = svm.SVC(**best_params99)\n",
    "        clf99.fit(x_99_train,y_train)\n",
    "        y_99_pred = clf99.predict(x_99_test)\n",
    "\n",
    "        y_pred_list = [y_99_pred]\n",
    "        x_test_list = [x_99_test]\n",
    "        clf_list = [clf99]\n",
    "        best_params_list =[best_params99]\n",
    "\n",
    "        #Generate and save classification report, macro-avg f1 score, confusion matrix\n",
    "        \n",
    "        target_names = ['T','C']\n",
    "        names_list = ['99']\n",
    "        for i in range(len(y_pred_list)):\n",
    "            df_name = file.split(\"\\\\\")[-1].rstrip('.pkl')+'_'+names_list[i]\n",
    "            report_name = file.split(\"\\\\\")[-1].rstrip('.pkl') + '_'+names_list[i]+'_PCA_CV_SVM_clf_report'\n",
    "            cm_name = file.split(\"\\\\\")[-1].rstrip('.pkl') + '_'+names_list[i]+'_PCA_CV_SVM_ConfusionMatrix'\n",
    "            ##Classification report \n",
    "            report = classification_report(y_test, y_pred_list[i], target_names=target_names, output_dict=True)\n",
    "            report_df = pd.DataFrame(report).transpose()\n",
    "            report_df.to_pickle(testdir+report_name+'.pkl')\n",
    "            ##Macro-avg f1 score \n",
    "            f1Score = report['macro avg']['f1-score']\n",
    "            test_results['F1 Score'].append(f1Score)\n",
    "            test_results['dataset'].append(df_name)\n",
    "            test_results['params'].append(best_params_list[i])\n",
    "            test_results_df = pd.DataFrame(test_results)\n",
    "            if bestF1['F1 Score']<f1Score:\n",
    "                bestF1['F1 Score'] = f1Score\n",
    "                bestF1['dataset'] = df_name\n",
    "                bestF1['params'] = str(best_params_list[i])\n",
    "            ##Confusion matrix\n",
    "            fig,ax = plt.subplots()\n",
    "            ax.set_title(df_name+' CM')\n",
    "            plot_confusion_matrix(clf_list[i],x_test_list[i],y_test,labels=target_names,ax=ax,normalize='true')\n",
    "            plt.savefig(testdir+df_name+'.png')\n",
    "            plt.close()\n",
    "\n",
    "    bestF1_df = pd.DataFrame(bestF1,index=[0],columns=['F1 Score','dataset','params'])\n",
    "    bestF1_df.to_csv(testdir+bestF1['dataset']+'.csv')\n",
    "    test_results_df.to_csv(testdir+'test_results.csv')\n",
    "    test_results_df.to_csv(testdir+'test_results.pkl')\n",
    "        \n",
    "\n",
    "\n",
    "apply_PCA_CV_SVM(loaddir,savedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bit4f00f251aa71407b905d36ad95b25cdd",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}