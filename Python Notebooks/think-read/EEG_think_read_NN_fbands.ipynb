{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the accuracies of each frequency band using NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pickle as pkl\n",
    "import itertools \n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "%matplotlib inline \n",
    "# %matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(320, 32)\n[-1025.9706589   -977.37886969  -696.16757795  -248.60606733\n   259.14171583   706.21097741   985.85916395  1031.01818106\n   830.43051956   431.46849735   -71.07012045  -557.52661941]\n"
    }
   ],
   "source": [
    "# Import filtered datasets \n",
    "\n",
    "# 0.05s \n",
    "#alpha\n",
    "t_alpha005 = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\filtered\\\\0.05s\\\\alpha/think.pkl\")\n",
    "r_alpha005 = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\filtered\\\\0.05s\\\\alpha/read.pkl\")\n",
    "#theta\n",
    "t_theta005 = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\filtered\\\\0.05s\\\\theta/think.pkl\")\n",
    "r_theta005 = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\filtered\\\\0.05s\\\\theta/read.pkl\")\n",
    "\n",
    "# 0.1s\n",
    "\n",
    "#alpha\n",
    "t_alpha01 = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\filtered\\\\0.1s\\\\alpha/think.pkl\")\n",
    "r_alpha01 = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\filtered\\\\0.1s\\\\alpha/read.pkl\")\n",
    "#theta\n",
    "t_theta01 = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\filtered\\\\0.1s\\\\theta/think.pkl\")\n",
    "r_theta01 = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\filtered\\\\0.1s\\\\theta/read.pkl\")\n",
    "\n",
    "print(t_alpha01.shape)\n",
    "print(r_alpha01.iloc[0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape each dataframe into No_samples,sampLen(6 or 12),32\n",
    "def reshape_df(df):\n",
    "    sampLen = len(df.iloc[0,0])\n",
    "    N = df.shape[0]\n",
    "    new_df = np.zeros((N,sampLen,32))\n",
    "    for i in range(32):\n",
    "        channel = df.iloc[:,i].values \n",
    "        channel_df = np.zeros((N,sampLen))\n",
    "        for j in range(len(channel)):\n",
    "            channel_df[j,:] = channel[j]\n",
    "        new_df[:,:,i] = keras.utils.normalize(channel_df)\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.05s \n",
    "\n",
    "#alpha\n",
    "t_alpha005r = reshape_df(t_alpha005)\n",
    "r_alpha005r = reshape_df(r_alpha005)\n",
    "#theta\n",
    "t_theta005r = reshape_df(t_theta005)\n",
    "r_theta005r = reshape_df(r_theta005)\n",
    "\n",
    "# 0.1s\n",
    "\n",
    "#alpha\n",
    "t_alpha01r = reshape_df(t_alpha01)\n",
    "r_alpha01r = reshape_df(r_alpha01)\n",
    "#theta\n",
    "t_theta01r = reshape_df(t_theta01)\n",
    "r_theta01r = reshape_df(r_theta01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(640, 6, 32)\n(640, 6, 32)\n(640, 6, 32)\n(640, 6, 32)\n(320, 12, 32)\n(320, 12, 32)\n(320, 12, 32)\n(320, 12, 32)\n"
    }
   ],
   "source": [
    "print(t_alpha005r.shape)\n",
    "print(r_alpha005r.shape)\n",
    "\n",
    "print(t_theta005r.shape)\n",
    "print(r_theta005r.shape)\n",
    "\n",
    "print(t_alpha01r.shape)\n",
    "print(r_alpha01r.shape)\n",
    "\n",
    "print(t_theta01r.shape)\n",
    "print(r_theta01r.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(640, 6, 64)\n(320, 12, 64)\n"
    }
   ],
   "source": [
    "#Combine theta and alpha bands \n",
    "#0.05s\n",
    "t_ta005 = np.concatenate((t_alpha005r,t_theta005r),axis=2)\n",
    "r_ta005 = np.concatenate((r_alpha005r,r_theta005r),axis=2)\n",
    "#0.1s\n",
    "t_ta01 = np.concatenate((t_alpha01r,t_theta01r),axis=2)\n",
    "r_ta01 = np.concatenate((r_alpha01r,r_theta01r),axis=2)\n",
    "\n",
    "print(r_ta005.shape)\n",
    "print(r_ta01.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(640, 12, 32)\n(320, 24, 32)\n"
    }
   ],
   "source": [
    "# Concat test\n",
    "#0.05s\n",
    "t_ta005t = np.concatenate((t_alpha005r,t_theta005r),axis=1)\n",
    "r_ta005t = np.concatenate((r_alpha005r,r_theta005r),axis=1)\n",
    "#0.1s\n",
    "t_ta01t = np.concatenate((t_alpha01r,t_theta01r),axis=1)\n",
    "r_ta01t = np.concatenate((r_alpha01r,r_theta01r),axis=1)\n",
    "\n",
    "print(t_ta005t.shape)\n",
    "print(t_ta01t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1280, 6, 64)\n(1280,)\n(640, 12, 64)\n(640,)\n"
    }
   ],
   "source": [
    "#0.05\n",
    "X005 = np.vstack((t_ta005,r_ta005))\n",
    "y005 = np.hstack((np.zeros(t_ta005.shape[0]),np.ones(r_ta005.shape[0])))\n",
    "print(X005.shape)\n",
    "print(y005.shape)\n",
    "#0.1\n",
    "X01 = np.vstack((t_ta01,r_ta01))\n",
    "y01 = np.hstack((np.zeros(t_ta01.shape[0]),np.ones(r_ta01.shape[0])))\n",
    "print(X01.shape)\n",
    "print(y01.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1280, 12, 32)\n(1280,)\n(640, 24, 32)\n(640,)\n"
    }
   ],
   "source": [
    "#CONCAT TEST\n",
    "#0.05\n",
    "X005t = np.vstack((t_ta005t,r_ta005t))\n",
    "y005t = np.hstack((np.zeros(t_ta005t.shape[0]),np.ones(r_ta005t.shape[0])))\n",
    "print(X005t.shape)\n",
    "print(y005t.shape)\n",
    "#0.1\n",
    "X01t = np.vstack((t_ta01t,r_ta01t))\n",
    "y01t = np.hstack((np.zeros(t_ta01t.shape[0]),np.ones(r_ta01t.shape[0])))\n",
    "print(X01t.shape)\n",
    "print(y01t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train and test sets\n",
    "def split_train_test(X,y):\n",
    "    sss = StratifiedShuffleSplit(n_splits=5,test_size=0.2,random_state=0)\n",
    "    for train_index, test_index in sss.split(X,y):\n",
    "                x_train, x_test = X[train_index],X[test_index]\n",
    "                y_train, y_test = y[train_index],y[test_index]\n",
    "    return x_train,x_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.05s\n",
    "x_train005,x_test005,y_train005,y_test005 = split_train_test(X005,y005)\n",
    "#0.1s\n",
    "x_train01,x_test01,y_train01,y_test01 = split_train_test(X01,y01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.05s\n",
    "x_train005t,x_test005t,y_train005t,y_test005t = split_train_test(X005t,y005t)\n",
    "#0.1s\n",
    "x_train01t,x_test01t,y_train01t,y_test01t = split_train_test(X01t,y01t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN model\n",
    "def DNN_model(sampLen,no_bands):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(sampLen*32*no_bands, input_shape = (sampLen,32*no_bands), activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"relu\"), \n",
    "        tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer='adam',\n",
    "                metrics=[\"accuracy\"])\n",
    "    # model.summary()\n",
    "    return model\n",
    "\n",
    "#Conv model\n",
    "def CONV_model(sampLen,no_bands):\n",
    "    model2 = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=128, kernel_size=5,strides=1, padding=\"causal\",\n",
    "                                activation=\"relu\",input_shape=(sampLen,32*no_bands)), #input shape = (n_timesteps,n_features)\n",
    "        tf.keras.layers.Conv1D(filters=64, kernel_size=5,strides=1, padding=\"causal\",activation=\"relu\"),\n",
    "        tf.keras.layers.Conv1D(filters=32, kernel_size=5,strides=1, padding=\"causal\",activation=\"relu\"),\n",
    "        # tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.MaxPool1D(pool_size=2),\n",
    "        tf.keras.layers.Conv1D(filters=16, kernel_size=5,strides=1, padding=\"causal\",activation=\"relu\"),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation=\"relu\"), \n",
    "        # tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "    ])\n",
    "    # optimizer = tf.keras.optimizers.Adam(lr=9e-4)\n",
    "    model2.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=\"adam\",\n",
    "                metrics=[\"accuracy\"])\n",
    "    return model2\n",
    "\n",
    "#Conv + LSTM\n",
    "def CONV_LSTM_model(sampLen,no_bands):\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=5,strides=1, padding=\"causal\",\n",
    "                              activation=\"relu\",input_shape=(sampLen,32*no_bands)), #input shape = (n_timesteps,n_features)\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=5,strides=1, padding=\"causal\",activation=\"relu\"),\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=5,strides=1, padding=\"causal\",activation=\"relu\"),\n",
    "    # tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.MaxPool1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=16, kernel_size=5,strides=1, padding=\"causal\",activation=\"relu\"),\n",
    "    # tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
    "    tf.keras.layers.Dense(10, activation=\"relu\"), \n",
    "    # tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "])\n",
    "    model3.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=[\"accuracy\"])\n",
    "    return model3\n",
    "\n",
    "#Simple RNN \n",
    "def RNN_model(sampLen,no_bands):\n",
    "    model4 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(40, input_shape = (sampLen,32*no_bands),return_sequences=True),\n",
    "    tf.keras.layers.SimpleRNN(40),\n",
    "    # tf.keras.layers.Dense(10,activation=\"relu\",input_shape=(12,32)),\n",
    "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "    ])\n",
    "    model4.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer='adam',\n",
    "                metrics=[\"accuracy\"])\n",
    "    return model4\n",
    "\n",
    "#LSTM only\n",
    "\n",
    "def LSTM_model(sampLen,no_bands):\n",
    "    model5 = tf.keras.models.Sequential([\n",
    "    # tf.keras.layers.Flatten(input_shape=(sampLen,32)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True,input_shape=(sampLen,32*no_bands))),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "    ])\n",
    "    model5.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer='adam',\n",
    "                metrics=[\"accuracy\"])\n",
    "    return model5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,dataset,devset,batchsize,epochs):\n",
    "    tf.random.set_seed(10)\n",
    "    datasetb = dataset.batch(batchsize)\n",
    "    devsetb = devset.batch(batchsize)\n",
    "    history = model.fit(datasetb,epochs=epochs,verbose=0,validation_data=devsetb)\n",
    "    return history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.05s \n",
    "dataset005 = tf.data.Dataset.from_tensor_slices((x_train005,y_train005))\n",
    "devset005  = tf.data.Dataset.from_tensor_slices((x_test005,y_test005))\n",
    "\n",
    "# 0.1s \n",
    "dataset01 = tf.data.Dataset.from_tensor_slices((x_train01,y_train01))\n",
    "devset01  = tf.data.Dataset.from_tensor_slices((x_test01,y_test01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.05s \n",
    "dataset005t = tf.data.Dataset.from_tensor_slices((x_train005t,y_train005t))\n",
    "devset005t  = tf.data.Dataset.from_tensor_slices((x_test005t,y_test005t))\n",
    "\n",
    "# 0.1s \n",
    "dataset01t = tf.data.Dataset.from_tensor_slices((x_train01t,y_train01t))\n",
    "devset01t  = tf.data.Dataset.from_tensor_slices((x_test01t,y_test01t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Layer bidirectional_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\n0.7011718153953552\n0.62890625\n0.6015625\n0.69921875\n0.55078125\n"
    }
   ],
   "source": [
    "# 0.05s \n",
    "# Testing different models, epochs=10, batchsize = 1 \n",
    "sampLen = 6 \n",
    "no_bands = 2\n",
    "batchsize = 1\n",
    "DNN_model005 = DNN_model(sampLen,no_bands)\n",
    "CONV_model005 = CONV_model(sampLen,no_bands)\n",
    "CONV_LSTM_model005 = CONV_LSTM_model(sampLen,no_bands)\n",
    "LSTM_model005 = LSTM_model(sampLen,no_bands)\n",
    "RNN_model005 = RNN_model(sampLen,no_bands)\n",
    "#Histories \n",
    "DNN_hist005 = test(DNN_model005,dataset005,devset005,batchsize,10)\n",
    "CONV_hist005 = test(CONV_model005,dataset005,devset005,batchsize,10)\n",
    "CONV_LSTM_hist005 = test(CONV_LSTM_model005,dataset005,devset005,batchsize,10)\n",
    "LSTM_hist005 = test(LSTM_model005,dataset005,devset005,batchsize,10)\n",
    "RNN_hist005 = test(RNN_model005,dataset005,devset005,batchsize,10)\n",
    "#Max validation accuracy \n",
    "valacc_DNN005 = max(DNN_hist005.history['val_accuracy'])\n",
    "valacc_CONV005 = max(CONV_hist005.history['val_accuracy'])\n",
    "valacc_CONVLSTM005 = max(CONV_LSTM_hist005.history['val_accuracy'])\n",
    "valacc_LSTM005 = max(LSTM_hist005.history['val_accuracy'])\n",
    "valacc_RNN005 = max(RNN_hist005.history['val_accuracy'])\n",
    "\n",
    "print(valacc_DNN005)\n",
    "print(valacc_CONV005)\n",
    "print(valacc_CONVLSTM005)\n",
    "print(valacc_LSTM005)\n",
    "print(valacc_RNN005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Layer bidirectional_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\n0.6445311307907104\n0.625\n0.5\n0.625\n0.5390625\n"
    }
   ],
   "source": [
    "# 0.1s \n",
    "# Testing different models, epochs=10, batchsize = 1 \n",
    "sampLen = 12\n",
    "no_bands = 2\n",
    "batchsize = 1\n",
    "DNN_model01 = DNN_model(sampLen,no_bands)\n",
    "CONV_model01 = CONV_model(sampLen,no_bands)\n",
    "CONV_LSTM_model01 = CONV_LSTM_model(sampLen,no_bands)\n",
    "LSTM_model01 = LSTM_model(sampLen,no_bands)\n",
    "RNN_model01 = RNN_model(sampLen,no_bands)\n",
    "#Histories \n",
    "DNN_hist01 = test(DNN_model01,dataset01,devset01,batchsize,10)\n",
    "CONV_hist01 = test(CONV_model01,dataset01,devset01,batchsize,10)\n",
    "CONV_LSTM_hist01 = test(CONV_LSTM_model01,dataset01,devset01,batchsize,10)\n",
    "LSTM_hist01 = test(LSTM_model01,dataset01,devset01,batchsize,10)\n",
    "RNN_hist01 = test(RNN_model01,dataset01,devset01,batchsize,10)\n",
    "#Max validation accuracy \n",
    "valacc_DNN01 = max(DNN_hist01.history['val_accuracy'])\n",
    "valacc_CONV01 = max(CONV_hist01.history['val_accuracy'])\n",
    "valacc_CONVLSTM01 = max(CONV_LSTM_hist01.history['val_accuracy'])\n",
    "valacc_LSTM01 = max(LSTM_hist01.history['val_accuracy'])\n",
    "valacc_RNN01 = max(RNN_hist01.history['val_accuracy'])\n",
    "\n",
    "print(valacc_DNN01)\n",
    "print(valacc_CONV01)\n",
    "print(valacc_CONVLSTM01)\n",
    "print(valacc_LSTM01)\n",
    "print(valacc_RNN01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Layer bidirectional_16 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\n0.6751301884651184\n0.73046875\n0.5\n0.73828125\n0.5859375\n"
    }
   ],
   "source": [
    "##CONCAT TEST\n",
    "# 0.05s \n",
    "# Testing different models, epochs=10, batchsize = 1 \n",
    "sampLen = 6*2 \n",
    "no_bands = 1\n",
    "batchsize = 1\n",
    "DNN_model005 = DNN_model(sampLen,no_bands)\n",
    "CONV_model005 = CONV_model(sampLen,no_bands)\n",
    "CONV_LSTM_model005 = CONV_LSTM_model(sampLen,no_bands)\n",
    "LSTM_model005 = LSTM_model(sampLen,no_bands)\n",
    "RNN_model005 = RNN_model(sampLen,no_bands)\n",
    "#Histories \n",
    "DNN_hist005 = test(DNN_model005,dataset005t,devset005t,batchsize,10)\n",
    "CONV_hist005 = test(CONV_model005,dataset005t,devset005t,batchsize,10)\n",
    "CONV_LSTM_hist005 = test(CONV_LSTM_model005,dataset005t,devset005t,batchsize,10)\n",
    "LSTM_hist005 = test(LSTM_model005,dataset005t,devset005t,batchsize,10)\n",
    "RNN_hist005 = test(RNN_model005,dataset005t,devset005t,batchsize,10)\n",
    "#Max validation accuracy \n",
    "valacc_DNN005 = max(DNN_hist005.history['val_accuracy'])\n",
    "valacc_CONV005 = max(CONV_hist005.history['val_accuracy'])\n",
    "valacc_CONVLSTM005 = max(CONV_LSTM_hist005.history['val_accuracy'])\n",
    "valacc_LSTM005 = max(LSTM_hist005.history['val_accuracy'])\n",
    "valacc_RNN005 = max(RNN_hist005.history['val_accuracy'])\n",
    "\n",
    "print(valacc_DNN005)\n",
    "print(valacc_CONV005)\n",
    "print(valacc_CONVLSTM005)\n",
    "print(valacc_LSTM005)\n",
    "print(valacc_RNN005)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bita8e6d15a408c4f89b0e1dbbe6fd633cc",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}