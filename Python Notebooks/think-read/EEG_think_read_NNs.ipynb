{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing multiple neural net architectures on time series data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pickle as pkl\n",
    "import itertools \n",
    "import glob\n",
    "from sklearn import svm \n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "%matplotlib inline \n",
    "# %matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(3840, 32)\n(3840, 32)\n"
    }
   ],
   "source": [
    "#Importing raw data files \n",
    "\n",
    "#.csv path\n",
    "csvpath = \"C:/Users/Wu Di/Documents/EEG-analysis/200108-Readings-csv/reading_switching.csv\"\n",
    "\n",
    "#Read .csv files\n",
    "cols_to_use = list(range(4, 36))\n",
    "\n",
    "#Raw dataframes - each channel is a column\n",
    "raw_df = pd.read_csv(csvpath, header=None, usecols=cols_to_use)\n",
    "\n",
    "#Split into thinking and counting data frames\n",
    "think_index_list = []\n",
    "read_index_list = []\n",
    "\n",
    "for i in range(6):\n",
    "    if i%2==0:\n",
    "        think_index_list+=(list(range(1280*i,1280*(i+1))))\n",
    "    else:\n",
    "        read_index_list+=(list(range(1280*i,1280*(i+1))))\n",
    "\n",
    "\n",
    "# for i in range(len(all_bands_list)):\n",
    "#     df = all_bands_list[i].iloc[0:1280*6]\n",
    "df = raw_df.iloc[0:1280*6]\n",
    "df_list = np.vsplit(df,6)\n",
    "think_df = pd.DataFrame(np.vstack((df_list[0],df_list[2],df_list[4])))\n",
    "read_df = pd.DataFrame(np.vstack((df_list[1],df_list[3],df_list[5])))\n",
    "\n",
    "print(think_df.shape)\n",
    "print(read_df.shape)\n",
    "\n",
    "think_df.to_pickle(\"F:\\EEG-data\\\\think-read\\\\raw/think.pkl\")\n",
    "read_df.to_pickle(\"F:\\EEG-data\\\\think-read\\\\raw/read.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(3840, 32)\n(3840, 32)\n-829.138184\n-206.888657\n"
    }
   ],
   "source": [
    "# Import raw datasets, \n",
    "think_df = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\raw/think.pkl\")\n",
    "read_df = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\raw/read.pkl\")\n",
    "\n",
    "print(think_df.shape)\n",
    "print(read_df.shape)\n",
    "print(think_df.iloc[0,0]) # Each element in dataframe is a single timestep\n",
    "print(read_df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits a single dataframe into list of equally sized arrays\n",
    "#Each element in list is nx32 array, where n= sample length \n",
    "def split_df(df,fs,sample_t,check=False):\n",
    "    rows,_ = df.shape #get no. of rows\n",
    "    sample_len = int(sample_t*fs) #find no. of recorded samples required for each sample time length\n",
    "    Ns = int(rows/sample_len) #find total no. of samples\n",
    "    df_cut = df.iloc[:Ns*sample_len] #truncate dataframe to exact multiple of sample length\n",
    "    # print(Ns*sample_len)\n",
    "    # print(Ns)\n",
    "    df_split_list = np.vsplit(df_cut,Ns) #split dataframe row-wise, returns a list\n",
    "    \n",
    "    if check:\n",
    "        print(\"Total no. of recorded samples: \"+str(rows))\n",
    "        print(\"Sample length: \"+str(sample_len))\n",
    "        print(\"Total no. of samples: \"+str(Ns))\n",
    "        print(\"Length of df_split_list: \"+str(len(df_split_list)))\n",
    "        \n",
    "        if all(isinstance(x.shape,tuple) for x in df_split_list):\n",
    "            print(\"Shape of each element in df_split_list: \"+str(df_split_list[0].shape))\n",
    "        else:\n",
    "            print(\"Shapes are wrong.\")\n",
    "            for x in df_split_list:\n",
    "                print(x.shape)\n",
    "    return df_split_list,Ns\n",
    "\n",
    "#Apply split_df() function to list of dataframes, reshape dataframe such that each element is an array \n",
    "#for the appropriate sample time length \n",
    "def split_bands_list(bands_list,fs,sample_t,check=False,checkSD=False):\n",
    "    df_list_rFE = [0]*len(bands_list) #dataframes list ready for feature extraction \n",
    "    for df_no in range(len(bands_list)):\n",
    "        df_split_list,Ns = split_df(bands_list[df_no],fs,sample_t,check=checkSD)\n",
    "        list_of_series = [0]*Ns\n",
    "        for i in range(len(df_split_list)):\n",
    "            #New dataframe will have shape Nsx32, each element is a 1xsample_len array \n",
    "            new_row = [0]*32 \n",
    "            #Each df_split_list[i] is a dataframe\n",
    "            for j in range(len(df_split_list[i].columns)):\n",
    "                new_row[j] = df_split_list[i].iloc[:,j].values \n",
    "            list_of_series[i] = new_row\n",
    "        df_list_rFE[df_no] = pd.DataFrame(list_of_series)\n",
    "    if check:\n",
    "        print(\"Length of bands_list: \"+str(len(bands_list)))\n",
    "        print(\"Length of df_list_rFE: \"+str(len(df_list_rFE)))\n",
    "        if (all(isinstance(x.shape,tuple) for x in df_list_rFE)) and (Ns==len(df_list_rFE[0].index)):\n",
    "            print(\"Shape of each dataframe in df_list_rFE: \"+str(df_list_rFE[0].shape))\n",
    "    return df_list_rFE\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.05s \n",
    "split005_t_df=split_bands_list([think_df],128,0.05)[0]\n",
    "split005_r_df=split_bands_list([read_df],128,0.05)[0]\n",
    "\n",
    "#0.1s \n",
    "split01_t_df=split_bands_list([think_df],128,0.1)[0]\n",
    "split01_r_df=split_bands_list([read_df],128,0.1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(640, 32)\n(640, 32)\n(320, 32)\n(320, 32)\n"
    }
   ],
   "source": [
    "print(split005_t_df.shape)\n",
    "print(split005_r_df.shape)\n",
    "\n",
    "print(split01_t_df.shape)\n",
    "print(split01_r_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save datasets\n",
    "split005_t_df.to_pickle(\"F:\\EEG-data\\\\think-read\\\\raw\\\\0.05s/raw_think.pkl\")\n",
    "split005_r_df.to_pickle(\"F:\\EEG-data\\\\think-read\\\\raw\\\\0.05s/raw_read.pkl\")\n",
    "\n",
    "split01_t_df.to_pickle(\"F:\\EEG-data\\\\think-read\\\\raw\\\\0.1s/raw_think.pkl\")\n",
    "split01_r_df.to_pickle(\"F:\\EEG-data\\\\think-read\\\\raw\\\\0.1s/raw_read.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "#0.05s \n",
    "t_df005 = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\raw\\\\0.05s/raw_think.pkl\")\n",
    "r_df005 = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\raw\\\\0.05s/raw_read.pkl\")\n",
    "\n",
    "#0.1s\n",
    "t_df01 = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\raw\\\\0.1s/raw_think.pkl\")\n",
    "r_df01 = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\raw\\\\0.1s/raw_read.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(640, 32)\n(640, 32)\n(320, 32)\n(320, 32)\n"
    }
   ],
   "source": [
    "print(t_df005.shape)\n",
    "print(r_df005.shape)\n",
    "\n",
    "print(t_df01.shape)\n",
    "print(r_df01.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILTERED \n",
    "# Import filtered datasets\n",
    "#0.05s \n",
    "ft_df005 = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\filtered\\\\0.05s/lp40.5_think.pkl\")\n",
    "fr_df005 = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\filtered\\\\0.05s/lp40.5_read.pkl\")\n",
    "\n",
    "#0.1s\n",
    "ft_df01 = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\filtered\\\\0.1s/lp40.5_think.pkl\")\n",
    "fr_df01 = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\filtered\\\\0.1s/lp40.5_read.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(640, 32)\n(640, 32)\n(320, 32)\n(320, 32)\n"
    }
   ],
   "source": [
    "print(ft_df005.shape)\n",
    "print(fr_df005.shape)\n",
    "\n",
    "print(ft_df01.shape)\n",
    "print(fr_df01.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape each dataframe into 320,12,32\n",
    "def reshape_df(df):\n",
    "    sampLen = len(df.iloc[0,0])\n",
    "    N = df.shape[0]\n",
    "    new_df = np.zeros((N,sampLen,32))\n",
    "    for i in range(32):\n",
    "        channel = df.iloc[:,i].values \n",
    "        channel_df = np.zeros((N,sampLen))\n",
    "        for j in range(len(channel)):\n",
    "            channel_df[j,:] = channel[j]\n",
    "        new_df[:,:,i] = keras.utils.normalize(channel_df)\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.05s\n",
    "t_df005r = reshape_df(t_df005)\n",
    "r_df005r = reshape_df(r_df005)\n",
    "\n",
    "#0.1s\n",
    "t_df01r = reshape_df(t_df01)\n",
    "r_df01r = reshape_df(r_df01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(640, 6, 32)\n(640, 6, 32)\n(320, 12, 32)\n(320, 12, 32)\n"
    }
   ],
   "source": [
    "#0.05s\n",
    "print(t_df005r.shape)\n",
    "print(r_df005r.shape)\n",
    "\n",
    "#0.1s\n",
    "print(t_df01r.shape)\n",
    "print(r_df01r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERED\n",
    "#0.05s\n",
    "ft_df005r = reshape_df(ft_df005)\n",
    "fr_df005r = reshape_df(fr_df005)\n",
    "\n",
    "#0.1s\n",
    "ft_df01r = reshape_df(ft_df01)\n",
    "fr_df01r = reshape_df(fr_df01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(640, 6, 32)\n(640, 6, 32)\n(320, 12, 32)\n(320, 12, 32)\n"
    }
   ],
   "source": [
    "## FILTERED\n",
    "#0.05s\n",
    "print(ft_df005r.shape)\n",
    "print(fr_df005r.shape)\n",
    "\n",
    "#0.1s\n",
    "print(ft_df01r.shape)\n",
    "print(fr_df01r.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1280, 6, 32)\n(1280,)\n(640, 12, 32)\n(640,)\n"
    }
   ],
   "source": [
    "#0.05\n",
    "X005 = np.vstack((t_df005r,r_df005r))\n",
    "y005 = np.hstack((np.zeros(t_df005r.shape[0]),np.ones(r_df005r.shape[0])))\n",
    "print(X005.shape)\n",
    "print(y005.shape)\n",
    "#0.1\n",
    "X01 = np.vstack((t_df01r,r_df01r))\n",
    "y01 = np.hstack((np.zeros(t_df01r.shape[0]),np.ones(r_df01r.shape[0])))\n",
    "print(X01.shape)\n",
    "print(y01.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1280, 6, 32)\n(1280,)\n(640, 12, 32)\n(640,)\n"
    }
   ],
   "source": [
    "#FILTERED \n",
    "#0.05\n",
    "fX005 = np.vstack((ft_df005r,fr_df005r))\n",
    "fy005 = np.hstack((np.zeros(ft_df005r.shape[0]),np.ones(fr_df005r.shape[0])))\n",
    "print(fX005.shape)\n",
    "print(fy005.shape)\n",
    "#0.1\n",
    "fX01 = np.vstack((ft_df01r,fr_df01r))\n",
    "fy01 = np.hstack((np.zeros(ft_df01r.shape[0]),np.ones(fr_df01r.shape[0])))\n",
    "print(fX01.shape)\n",
    "print(fy01.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train and test sets\n",
    "def split_train_test(X,y):\n",
    "    sss = StratifiedShuffleSplit(n_splits=5,test_size=0.2,random_state=0)\n",
    "    for train_index, test_index in sss.split(X,y):\n",
    "                x_train, x_test = X[train_index],X[test_index]\n",
    "                y_train, y_test = y[train_index],y[test_index]\n",
    "    return x_train,x_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN model\n",
    "def DNN_model(sampLen):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(sampLen*32, input_shape = (sampLen,32), activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"relu\"), \n",
    "        tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer='adam',\n",
    "                metrics=[\"accuracy\"])\n",
    "    # model.summary()\n",
    "    return model\n",
    "\n",
    "#Conv model\n",
    "def CONV_model(sampLen):\n",
    "    model2 = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=128, kernel_size=5,strides=1, padding=\"causal\",\n",
    "                                activation=\"relu\",input_shape=(sampLen,32)), #input shape = (n_timesteps,n_features)\n",
    "        tf.keras.layers.Conv1D(filters=64, kernel_size=5,strides=1, padding=\"causal\",activation=\"relu\"),\n",
    "        tf.keras.layers.Conv1D(filters=32, kernel_size=5,strides=1, padding=\"causal\",activation=\"relu\"),\n",
    "        # tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.MaxPool1D(pool_size=2),\n",
    "        tf.keras.layers.Conv1D(filters=16, kernel_size=5,strides=1, padding=\"causal\",activation=\"relu\"),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation=\"relu\"), \n",
    "        # tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "    ])\n",
    "    # optimizer = tf.keras.optimizers.Adam(lr=9e-4)\n",
    "    model2.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=\"adam\",\n",
    "                metrics=[\"accuracy\"])\n",
    "    return model2\n",
    "\n",
    "#Conv + LSTM\n",
    "def CONV_LSTM_model(sampLen):\n",
    "    model3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=5,strides=1, padding=\"causal\",\n",
    "                              activation=\"relu\",input_shape=(sampLen,32)), #input shape = (n_timesteps,n_features)\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=5,strides=1, padding=\"causal\",activation=\"relu\"),\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=5,strides=1, padding=\"causal\",activation=\"relu\"),\n",
    "    # tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.MaxPool1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=16, kernel_size=5,strides=1, padding=\"causal\",activation=\"relu\"),\n",
    "    # tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
    "    tf.keras.layers.Dense(10, activation=\"relu\"), \n",
    "    # tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "])\n",
    "    model3.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=[\"accuracy\"])\n",
    "    return model3\n",
    "\n",
    "#Simple RNN \n",
    "def RNN_model(sampLen):\n",
    "    model4 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(40, input_shape = (sampLen,32),return_sequences=True),\n",
    "    tf.keras.layers.SimpleRNN(40),\n",
    "    # tf.keras.layers.Dense(10,activation=\"relu\",input_shape=(12,32)),\n",
    "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "    ])\n",
    "    model4.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer='adam',\n",
    "                metrics=[\"accuracy\"])\n",
    "    return model4\n",
    "\n",
    "#LSTM only\n",
    "\n",
    "def LSTM_model(sampLen):\n",
    "    model5 = tf.keras.models.Sequential([\n",
    "    # tf.keras.layers.Flatten(input_shape=(sampLen,32)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True,input_shape=(sampLen,32))),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "    ])\n",
    "    model5.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer='adam',\n",
    "                metrics=[\"accuracy\"])\n",
    "    return model5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,dataset,devset,batchsize,epochs):\n",
    "    tf.random.set_seed(10)\n",
    "    datasetb = dataset.batch(batchsize)\n",
    "    devsetb = devset.batch(batchsize)\n",
    "    history = model.fit(datasetb,epochs=epochs,verbose=0,validation_data=devsetb)\n",
    "    return history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.05s\n",
    "x_train005,x_test005,y_train005,y_test005 = split_train_test(X005,y005)\n",
    "#0.1s\n",
    "x_train01,x_test01,y_train01,y_test01 = split_train_test(X01,y01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.05s \n",
    "dataset005 = tf.data.Dataset.from_tensor_slices((x_train005,y_train005))\n",
    "devset005  = tf.data.Dataset.from_tensor_slices((x_test005,y_test005))\n",
    "dataset005b = dataset005.batch(40)\n",
    "devset005b = devset005.batch(40)\n",
    "\n",
    "# 0.1s \n",
    "dataset01 = tf.data.Dataset.from_tensor_slices((x_train01,y_train01))\n",
    "devset01  = tf.data.Dataset.from_tensor_slices((x_test01,y_test01))\n",
    "dataset01b = dataset01.batch(40)\n",
    "devset01b = devset01.batch(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Layer bidirectional_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\n0.9140625\n0.90234375\n0.90234375\n0.89453125\n0.83984375\n"
    }
   ],
   "source": [
    "# 0.05s \n",
    "# Testing different models, epochs=10, batchsize = 1 \n",
    "sampLen = 6 \n",
    "DNN_model005 = DNN_model(sampLen)\n",
    "CONV_model005 = CONV_model(sampLen)\n",
    "CONV_LSTM_model005 = CONV_LSTM_model(sampLen)\n",
    "LSTM_model005 = LSTM_model(sampLen)\n",
    "RNN_model005 = RNN_model(sampLen)\n",
    "#Histories \n",
    "DNN_hist005 = test(DNN_model005,dataset005,devset005,1,10)\n",
    "CONV_hist005 = test(CONV_model005,dataset005,devset005,1,10)\n",
    "CONV_LSTM_hist005 = test(CONV_LSTM_model005,dataset005,devset005,1,10)\n",
    "LSTM_hist005 = test(LSTM_model005,dataset005,devset005,1,10)\n",
    "RNN_hist005 = test(RNN_model005,dataset005,devset005,1,10)\n",
    "#Max validation accuracy \n",
    "valacc_DNN005 = max(DNN_hist005.history['val_accuracy'])\n",
    "valacc_CONV005 = max(CONV_hist005.history['val_accuracy'])\n",
    "valacc_CONVLSTM005 = max(CONV_LSTM_hist005.history['val_accuracy'])\n",
    "valacc_LSTM005 = max(LSTM_hist005.history['val_accuracy'])\n",
    "valacc_RNN005 = max(RNN_hist005.history['val_accuracy'])\n",
    "\n",
    "print(valacc_DNN005)\n",
    "print(valacc_CONV005)\n",
    "print(valacc_CONVLSTM005)\n",
    "print(valacc_LSTM005)\n",
    "print(valacc_RNN005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Layer bidirectional_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\n0.8606771230697632\n0.90625\n0.5\n0.9375\n0.8515625\n"
    }
   ],
   "source": [
    "# 0.1s \n",
    "# Testing different models, epochs=10, batchsize = 1 \n",
    "sampLen = 12\n",
    "DNN_model01 = DNN_model(sampLen)\n",
    "CONV_model01 = CONV_model(sampLen)\n",
    "CONV_LSTM_model01 = CONV_LSTM_model(sampLen)\n",
    "LSTM_model01 = LSTM_model(sampLen)\n",
    "RNN_model01 = RNN_model(sampLen)\n",
    "#Histories \n",
    "DNN_hist01 = test(DNN_model01,dataset01,devset01,1,10)\n",
    "CONV_hist01 = test(CONV_model01,dataset01,devset01,1,10)\n",
    "CONV_LSTM_hist01 = test(CONV_LSTM_model01,dataset01,devset01,1,10)\n",
    "LSTM_hist01 = test(LSTM_model01,dataset01,devset01,1,10)\n",
    "RNN_hist01 = test(RNN_model01,dataset01,devset01,1,10)\n",
    "#Max validation accuracy \n",
    "valacc_DNN01 = max(DNN_hist01.history['val_accuracy'])\n",
    "valacc_CONV01 = max(CONV_hist01.history['val_accuracy'])\n",
    "valacc_CONVLSTM01 = max(CONV_LSTM_hist01.history['val_accuracy'])\n",
    "valacc_LSTM01 = max(LSTM_hist01.history['val_accuracy'])\n",
    "valacc_RNN01 = max(RNN_hist01.history['val_accuracy'])\n",
    "\n",
    "print(valacc_DNN01)\n",
    "print(valacc_CONV01)\n",
    "print(valacc_CONVLSTM01)\n",
    "print(valacc_LSTM01)\n",
    "print(valacc_RNN01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERED\n",
    "#0.05s\n",
    "fx_train005,fx_test005,fy_train005,fy_test005 = split_train_test(fX005,fy005)\n",
    "#0.1s\n",
    "fx_train01,fx_test01,fy_train01,fy_test01 = split_train_test(fX01,fy01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILTERED\n",
    "# 0.05s \n",
    "fdataset005 = tf.data.Dataset.from_tensor_slices((fx_train005,fy_train005))\n",
    "fdevset005  = tf.data.Dataset.from_tensor_slices((fx_test005,fy_test005))\n",
    "fdataset005b = fdataset005.batch(40)\n",
    "fdevset005b = fdevset005.batch(40)\n",
    "\n",
    "# 0.1s \n",
    "fdataset01 = tf.data.Dataset.from_tensor_slices((fx_train01,fy_train01))\n",
    "fdevset01  = tf.data.Dataset.from_tensor_slices((fx_test01,fy_test01))\n",
    "fdataset01b = fdataset01.batch(40)\n",
    "fdevset01b = fdevset01.batch(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Layer bidirectional_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\nFiltered validation accuracies\n0.8522133827209473\n0.90234375\n0.90625\n0.89453125\n0.828125\n"
    }
   ],
   "source": [
    "#### FILTERED ####\n",
    "# 0.05s \n",
    "# Testing different models, epochs=10, batchsize = 1 \n",
    "sampLen = 6 \n",
    "DNN_model005 = DNN_model(sampLen)\n",
    "CONV_model005 = CONV_model(sampLen)\n",
    "CONV_LSTM_model005 = CONV_LSTM_model(sampLen)\n",
    "LSTM_model005 = LSTM_model(sampLen)\n",
    "RNN_model005 = RNN_model(sampLen)\n",
    "#Histories \n",
    "DNN_hist005f = test(DNN_model005,fdataset005,fdevset005,1,10)\n",
    "CONV_hist005f = test(CONV_model005,fdataset005,fdevset005,1,10)\n",
    "CONV_LSTM_hist005f = test(CONV_LSTM_model005,fdataset005,fdevset005,1,10)\n",
    "LSTM_hist005f = test(LSTM_model005,fdataset005,fdevset005,1,10)\n",
    "RNN_hist005f = test(RNN_model005,fdataset005,fdevset005,1,10)\n",
    "#Max validation accuracy \n",
    "valacc_DNN005f = max(DNN_hist005f.history['val_accuracy'])\n",
    "valacc_CONV005f = max(CONV_hist005f.history['val_accuracy'])\n",
    "valacc_CONVLSTM005f = max(CONV_LSTM_hist005f.history['val_accuracy'])\n",
    "valacc_LSTM005f = max(LSTM_hist005f.history['val_accuracy'])\n",
    "valacc_RNN005f = max(RNN_hist005f.history['val_accuracy'])\n",
    "\n",
    "print(\"Filtered validation accuracies\")\n",
    "print(valacc_DNN005f)\n",
    "print(valacc_CONV005f)\n",
    "print(valacc_CONVLSTM005f)\n",
    "print(valacc_LSTM005f)\n",
    "print(valacc_RNN005f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Layer bidirectional_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\nFiltered validation accuracies\n0.8736978769302368\n0.8984375\n0.5\n0.9296875\n0.796875\n"
    }
   ],
   "source": [
    "##### FILTERED ########\n",
    "#0.1s\n",
    "# Testing different models, epochs=10, batchsize = 1 \n",
    "sampLen = 12\n",
    "DNN_model01 = DNN_model(sampLen)\n",
    "CONV_model01 = CONV_model(sampLen)\n",
    "CONV_LSTM_model01 = CONV_LSTM_model(sampLen)\n",
    "LSTM_model01 = LSTM_model(sampLen)\n",
    "RNN_model01 = RNN_model(sampLen)\n",
    "#Histories \n",
    "DNN_hist01f = test(DNN_model01,fdataset01,fdevset01,1,10)\n",
    "CONV_hist01f = test(CONV_model01,fdataset01,fdevset01,1,10)\n",
    "CONV_LSTM_hist01f = test(CONV_LSTM_model01,fdataset01,fdevset01,1,10)\n",
    "LSTM_hist01f = test(LSTM_model01,fdataset01,fdevset01,1,10)\n",
    "RNN_hist01f = test(RNN_model01,fdataset01,fdevset01,1,10)\n",
    "#Max validation accuracy \n",
    "valacc_DNN01f = max(DNN_hist01f.history['val_accuracy'])\n",
    "valacc_CONV01f = max(CONV_hist01f.history['val_accuracy'])\n",
    "valacc_CONVLSTM01f = max(CONV_LSTM_hist01f.history['val_accuracy'])\n",
    "valacc_LSTM01f = max(LSTM_hist01f.history['val_accuracy'])\n",
    "valacc_RNN01f = max(RNN_hist01f.history['val_accuracy'])\n",
    "\n",
    "print(\"Filtered validation accuracies\")\n",
    "print(valacc_DNN01f)\n",
    "print(valacc_CONV01f)\n",
    "print(valacc_CONVLSTM01f)\n",
    "print(valacc_LSTM01f)\n",
    "print(valacc_RNN01f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing different batch sizes \n",
    "def test_batchsizes(model,dataset,devset,batchsizes,epochs):\n",
    "    histories = []\n",
    "    for batchsize in batchsizes:\n",
    "        histories.append(test(model,dataset,devset,batchsize,epochs))\n",
    "    return histories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-0e39576a9c41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mCONV_LSTM_01_histories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_batchsizes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCONV_LSTM_model01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevset01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchsizes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mLSTM_01_histories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_batchsizes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM_model01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevset01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchsizes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mRNN01_histories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_batchsizes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN_model01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevset01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchsizes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-081896d723a2>\u001b[0m in \u001b[0;36mtest_batchsizes\u001b[1;34m(model, dataset, devset, batchsizes, epochs)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mhistories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatchsize\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatchsizes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mhistories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhistories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-4555322856dd>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(model, dataset, devset, batchsize, epochs)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdatasetb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdevsetb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasetb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevsetb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 0.1s \n",
    "batchsizes = list(range(1,40,10))\n",
    "\n",
    "DNN01_histories = test_batchsizes(DNN_model01,dataset01,devset01,batchsizes,10)\n",
    "CONV01_histories = test_batchsizes(CONV_model01,dataset01,devset01,batchsizes,10)\n",
    "CONV_LSTM_01_histories = test_batchsizes(CONV_LSTM_model01,dataset01,devset01,batchsizes,10)\n",
    "LSTM_01_histories = test_batchsizes(LSTM_model01,dataset01,devset01,batchsizes,10)\n",
    "RNN01_histories = test_batchsizes(RNN_model01,dataset01,devset01,batchsizes,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "batch size: 31, val acc: 0.8860676884651184\nbatch size: 1, val acc: 0.9140625\nbatch size: 1, val acc: 0.5\nbatch size: 11, val acc: 0.90625\nbatch size: 21, val acc: 0.890625\n"
    }
   ],
   "source": [
    "hists_maxvalacc = lambda histories: max([history.history['val_accuracy'][-1] for history in histories])\n",
    "hists_maxvalacc_batch = lambda histories,batchsizes: batchsizes[np.argmax(np.array([history.history['val_accuracy'][-1] for history in histories]))]\n",
    "\n",
    "print(\"batch size: {}, val acc: {}\".format(hists_maxvalacc_batch(DNN01_histories,batchsizes),hists_maxvalacc(DNN01_histories)))\n",
    "print(\"batch size: {}, val acc: {}\".format(hists_maxvalacc_batch(CONV01_histories,batchsizes),hists_maxvalacc(CONV01_histories)))\n",
    "print(\"batch size: {}, val acc: {}\".format(hists_maxvalacc_batch(CONV_LSTM_01_histories,batchsizes),hists_maxvalacc(CONV_LSTM_01_histories)))\n",
    "print(\"batch size: {}, val acc: {}\".format(hists_maxvalacc_batch(LSTM_01_histories,batchsizes),hists_maxvalacc(LSTM_01_histories)))\n",
    "print(\"batch size: {}, val acc: {}\".format(hists_maxvalacc_batch(RNN01_histories,batchsizes),hists_maxvalacc(RNN01_histories)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FILTERING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(3840, 32)\n(3840, 32)\n"
    }
   ],
   "source": [
    "#Import raw recordings\n",
    "\n",
    "think_df = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\raw/think.pkl\")\n",
    "read_df = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\raw/read.pkl\")\n",
    "\n",
    "print(think_df.shape)\n",
    "print(read_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bandpass (BP) filter helper functions\n",
    "\n",
    "#Creates butterworth BP filter\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5*fs  # Nyquist frequency, which is half of fs\n",
    "    low = lowcut/nyq  # Digital butterworth filter cutoffs must be normalized to Nyquist frequency\n",
    "    high = highcut/nyq\n",
    "    b, a = signal.butter(order, [low, high], btype=\"bandpass\")\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass(cutFreq,fs,order=5):\n",
    "    nyq = 0.5*fs\n",
    "    cutFreq = cutFreq/nyq\n",
    "    b,a = signal.butter(order,cutFreq,btype=\"lowpass\")\n",
    "    return b,a \n",
    "\n",
    "def butter_highpass(cutFreq,fs,order=5):\n",
    "    nyq = 0.5*fs\n",
    "    cutFreq = cutFreq/nyq\n",
    "    b,a = signal.butter(order,cutFreq,btype=\"highpass\")\n",
    "    return b,a \n",
    "\n",
    "#Applies butterworth BP filter\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "#     filtered_data = signal.lfilter(b, a, data)\n",
    "    filtered_data = signal.filtfilt(b,a,data)\n",
    "    return filtered_data\n",
    "\n",
    "#Applies butterworth lowpass filter\n",
    "def butter_lowpass_filter(data, cutFreq, fs, order=5):\n",
    "    b, a = butter_lowpass(cutFreq,fs,order=5)\n",
    "    filtered_data = signal.filtfilt(b,a,data)\n",
    "    return filtered_data\n",
    "\n",
    "#Applies butterworth lowpass filter\n",
    "def butter_highpass_filter(data, cutFreq, fs, order=5):\n",
    "    b, a = butter_highpass(cutFreq,fs,order=5)\n",
    "    filtered_data = signal.filtfilt(b,a,data)\n",
    "    return filtered_data\n",
    "\n",
    "#Applies butterworth BP filter to Pandas dataframe \n",
    "def bp_filter_df(df, lowcut, highcut, fs, order):\n",
    "    rows, cols = df.shape  # Get no. of rows and cols in df\n",
    "    new_index = range(1, rows+1)\n",
    "    new_cols = range(1, cols+1)\n",
    "    # Create new df with same no. of rows and cols\n",
    "    new_df = pd.DataFrame(index=new_index, columns=new_cols)\n",
    "    # new_df = new_df.fillna(0) #Fill in 0 for all values\n",
    "    for i in range(cols):  # Apply bp filter each column (channel) and saves in new_df\n",
    "        filt_col = butter_bandpass_filter(\n",
    "            df.iloc[:, i].values, lowcut, highcut, fs, order)\n",
    "        new_df[i+1] = filt_col\n",
    "    return new_df\n",
    "\n",
    "#Applies butterworth lowpass filter to Pandas dataframe \n",
    "def lp_filter_df(df, cutFreq, fs, order):\n",
    "    rows, cols = df.shape  # Get no. of rows and cols in df\n",
    "    new_index = range(1, rows+1)\n",
    "    new_cols = range(1, cols+1)\n",
    "    # Create new df with same no. of rows and cols\n",
    "    new_df = pd.DataFrame(index=new_index, columns=new_cols)\n",
    "    # new_df = new_df.fillna(0) #Fill in 0 for all values\n",
    "    for i in range(cols):  # Apply bp filter each column (channel) and saves in new_df\n",
    "        filt_col = butter_lowpass_filter(\n",
    "            df.iloc[:, i].values, cutFreq, fs, order)\n",
    "        new_df[i+1] = filt_col\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_think_df = lp_filter_df(think_df,40.5,128,6)\n",
    "filt_read_df = lp_filter_df(read_df,40.5,128,6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits a single dataframe into list of equally sized arrays\n",
    "#Each element in list is nx32 array, where n= sample length \n",
    "def split_df(df,fs,sample_t,check=False):\n",
    "    rows,_ = df.shape #get no. of rows\n",
    "    sample_len = int(sample_t*fs) #find no. of recorded samples required for each sample time length\n",
    "    Ns = int(rows/sample_len) #find total no. of samples\n",
    "    df_cut = df.iloc[:Ns*sample_len] #truncate dataframe to exact multiple of sample length\n",
    "    # print(Ns*sample_len)\n",
    "    # print(Ns)\n",
    "    df_split_list = np.vsplit(df_cut,Ns) #split dataframe row-wise, returns a list\n",
    "    \n",
    "    if check:\n",
    "        print(\"Total no. of recorded samples: \"+str(rows))\n",
    "        print(\"Sample length: \"+str(sample_len))\n",
    "        print(\"Total no. of samples: \"+str(Ns))\n",
    "        print(\"Length of df_split_list: \"+str(len(df_split_list)))\n",
    "        \n",
    "        if all(isinstance(x.shape,tuple) for x in df_split_list):\n",
    "            print(\"Shape of each element in df_split_list: \"+str(df_split_list[0].shape))\n",
    "        else:\n",
    "            print(\"Shapes are wrong.\")\n",
    "            for x in df_split_list:\n",
    "                print(x.shape)\n",
    "    return df_split_list,Ns\n",
    "\n",
    "#Apply split_df() function to list of dataframes, reshape dataframe such that each element is an array \n",
    "#for the appropriate sample time length \n",
    "def split_bands_list(bands_list,fs,sample_t,check=False,checkSD=False):\n",
    "    df_list_rFE = [0]*len(bands_list) #dataframes list ready for feature extraction \n",
    "    for df_no in range(len(bands_list)):\n",
    "        df_split_list,Ns = split_df(bands_list[df_no],fs,sample_t,check=checkSD)\n",
    "        list_of_series = [0]*Ns\n",
    "        for i in range(len(df_split_list)):\n",
    "            #New dataframe will have shape Nsx32, each element is a 1xsample_len array \n",
    "            new_row = [0]*32 \n",
    "            #Each df_split_list[i] is a dataframe\n",
    "            for j in range(len(df_split_list[i].columns)):\n",
    "                new_row[j] = df_split_list[i].iloc[:,j].values \n",
    "            list_of_series[i] = new_row\n",
    "        df_list_rFE[df_no] = pd.DataFrame(list_of_series)\n",
    "    if check:\n",
    "        print(\"Length of bands_list: \"+str(len(bands_list)))\n",
    "        print(\"Length of df_list_rFE: \"+str(len(df_list_rFE)))\n",
    "        if (all(isinstance(x.shape,tuple) for x in df_list_rFE)) and (Ns==len(df_list_rFE[0].index)):\n",
    "            print(\"Shape of each dataframe in df_list_rFE: \"+str(df_list_rFE[0].shape))\n",
    "    return df_list_rFE\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.05s filt\n",
    "fsplit005_t_df=split_bands_list([filt_think_df],128,0.05)[0]\n",
    "fsplit005_r_df=split_bands_list([filt_read_df],128,0.05)[0]\n",
    "\n",
    "#0.1s filt\n",
    "fsplit01_t_df=split_bands_list([filt_think_df],128,0.1)[0]\n",
    "fsplit01_r_df=split_bands_list([filt_read_df],128,0.1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(640, 32)\n(640, 32)\n(320, 32)\n(320, 32)\n"
    }
   ],
   "source": [
    "print(fsplit005_t_df.shape)\n",
    "print(fsplit005_r_df.shape)\n",
    "\n",
    "print(fsplit01_t_df.shape)\n",
    "print(fsplit01_r_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save datasets\n",
    "# Filt \n",
    "fsplit005_t_df.to_pickle(\"F:\\EEG-data\\\\think-read\\\\filtered\\\\0.05s/lp40.5_think.pkl\")\n",
    "fsplit005_r_df.to_pickle(\"F:\\EEG-data\\\\think-read\\\\filtered\\\\0.05s/lp40.5_read.pkl\")\n",
    "\n",
    "fsplit01_t_df.to_pickle(\"F:\\EEG-data\\\\think-read\\\\filtered\\\\0.1s/lp40.5_think.pkl\")\n",
    "fsplit01_r_df.to_pickle(\"F:\\EEG-data\\\\think-read\\\\filtered\\\\0.1s/lp40.5_read.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bit4f00f251aa71407b905d36ad95b25cdd",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}