{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining channel importance\n",
    "PSD features, theta and alpha bands only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pickle as pkl\n",
    "import itertools \n",
    "import glob\n",
    "from sklearn import svm \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# %matplotlib inline \n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset\n",
    "t_auc_df = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\featureExtraction\\AUC_only/AUC_df_1.pkl\")\n",
    "t_psd_df = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\featureExtraction\\PSD_only/PSD_df_1.pkl\")\n",
    "a_auc_df = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\featureExtraction\\AUC_only/AUC_df_2.pkl\")\n",
    "a_psd_df = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\\\featureExtraction\\PSD_only/PSD_df_2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_SVM_channel(auc_df,psd_df):\n",
    "    channels_list = []\n",
    "    f1_scores = []\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=5,test_size=0.2,random_state=0)\n",
    "    # bestF1 = {'F1 Score':0,'dataset':'','params':''}\n",
    "    # test_results = {'F1 Score':[],'dataset':[],'params':[]}\n",
    "    #Create PCA instances\n",
    "    pca99 = PCA(n_components=0.99, svd_solver='full')\n",
    "    #Instantiate SVM gridsearch\n",
    "    C_range = [1,3,10,30,100,300,1000]\n",
    "    param_grid= [\n",
    "                # {'C': C_range, 'kernel': ['linear']},\n",
    "                {'C': C_range, 'gamma': [0.001, 0.0001, 'auto', 'scale'], 'kernel': ['rbf']},\n",
    "                ]\n",
    "    scoring = {'f1_macro'}\n",
    "    clf = svm.SVC()\n",
    "    grid = GridSearchCV(clf,param_grid=param_grid,scoring=scoring,cv=5,refit='f1_macro')\n",
    "\n",
    "    for i in range(32):\n",
    "        #Select columns \n",
    "        y = psd_df.iloc[:,-1].values\n",
    "        y1 = auc_df.iloc[:,-1].values\n",
    "        psd_col_select = lambda y: [x for x in list(psd_df.columns) if \"\".join((\"Ch\",str(y),\"_\")) in str(x)]\n",
    "        X = psd_df[psd_col_select(i+1)].values\n",
    "        X1 = auc_df.iloc[:,i].values.reshape(-1,1)\n",
    "        #Split into training and test sets\n",
    "        for train_index, test_index in sss.split(X,y):\n",
    "            X_train, X_test = X[train_index],X[test_index]\n",
    "            y_train, y_test = y[train_index],y[test_index]\n",
    "            X1_train, X1_test = X1[train_index],X1[test_index]\n",
    "            y1_train, y1_test = y1[train_index],y1[test_index]\n",
    "        #Fit transform on training data\n",
    "        x_99_train = pca99.fit_transform(X_train)\n",
    "        #Fit transform on test data\n",
    "        x_99_test = pca99.transform(X_test)\n",
    "        #Gridsearch\n",
    "        grid99 = grid.fit(x_99_train,y_train)\n",
    "        grid991 = grid.fit(X1_train,y1_train)\n",
    "        #Classifiers \n",
    "        clf99 = svm.SVC(**grid99.best_params_)\n",
    "        clf991 = svm.SVC(**grid991.best_params_)\n",
    "        clf99.fit(x_99_train,y_train)\n",
    "        clf991.fit(X1_train,y1_train)\n",
    "        #y_pred\n",
    "        y_99_pred = clf99.predict(x_99_test)\n",
    "        y_991_pred = clf991.predict(X1_test)\n",
    "        #F1 Score \n",
    "        f1Score = f1_score(y_test,y_99_pred,pos_label='T')\n",
    "        f1Score1 = f1_score(y1_test,y_991_pred,pos_label='T')\n",
    "        f1_scores.append(f1Score)\n",
    "        f1_scores.append(f1Score1)\n",
    "        #Channel list\n",
    "        channels_list.append(\"\".join((\"Ch\",str(i+1))))\n",
    "        channels_list.append(\"\".join((\"aCh\",str(i+1))))\n",
    "    #Order lists \n",
    "    \n",
    "    return channels_list,f1_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_channels,t_f1_scores = apply_SVM_channel(t_auc_df,t_psd_df)\n",
    "a_channels,a_f1_scores = apply_SVM_channel(a_auc_df,a_psd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the channels in descending order of F1 scores \n",
    "add_band = lambda channels_list,y: [\"\".join((x,\"_\",y)) for x in channels_list]\n",
    "t_channels1 = add_band(t_channels,\"t\")\n",
    "a_channels1 = add_band(a_channels,\"a\")\n",
    "\n",
    "channels_list = np.array(t_channels1 + a_channels1)\n",
    "f1_scores = np.array(t_f1_scores + a_f1_scores)\n",
    "\n",
    "sorted_channels = channels_list[f1_scores.argsort()[::-1]] #in descending order, best scores first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "128\n"
    }
   ],
   "source": [
    "print(len(sorted_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bestCh_df(t_auc_df,a_auc_df,t_psd_df,a_psd_df,sorted_channels,no_channels):\n",
    "    channels = sorted_channels[:no_channels]\n",
    "    df_list = [0]*no_channels\n",
    "    for (i,channel) in enumerate(channels):\n",
    "        if channel[-1] == \"t\":\n",
    "            if channel[0] == \"a\": #auc \n",
    "                channel_no = channel[channel.find(\"Ch\")+len(\"Ch\"):channel.find(\"_\")]\n",
    "                df_list[i] = t_auc_df.iloc[:,int(channel_no)-1]\n",
    "            else: #psd\n",
    "                channel_no = channel[channel.find(\"Ch\")+len(\"Ch\"):channel.find(\"_\")]\n",
    "                psd_col_select = lambda y: [x for x in list(t_psd_df.columns) if \"\".join((\"Ch\",str(y),\"_\")) in str(x)]\n",
    "                df_list[i] = t_psd_df[psd_col_select(channel_no)]\n",
    "        elif channel[-1] == \"a\":\n",
    "            if channel[0] == \"a\": #auc \n",
    "                channel_no = channel[channel.find(\"Ch\")+len(\"Ch\"):channel.find(\"_\")]\n",
    "                df_list[i] = a_auc_df.iloc[:,int(channel_no)-1]\n",
    "            else: #psd\n",
    "                channel_no = channel[channel.find(\"Ch\")+len(\"Ch\"):channel.find(\"_\")]\n",
    "                psd_col_select = lambda y: [x for x in list(a_psd_df.columns) if \"\".join((\"Ch\",str(y),\"_\")) in str(x)]\n",
    "                df_list[i] = a_psd_df[psd_col_select(channel_no)]\n",
    "    df_list.append(t_auc_df.iloc[:,-1])\n",
    "    return pd.concat(df_list,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_CV_SVM(bestChdf):\n",
    "    sss = StratifiedShuffleSplit(n_splits=5,test_size=0.2,random_state=0)\n",
    "    # bestF1 = {'F1 Score':0,'dataset':'','params':''}\n",
    "    # test_results = {'F1 Score':[],'dataset':[],'params':[]}\n",
    "    #Create PCA instances\n",
    "    pca99 = PCA(n_components=0.999, svd_solver='full')\n",
    "    #Instantiate SVM gridsearch\n",
    "    C_range = [1,3,10,30,100,300,1000]\n",
    "    param_grid= [\n",
    "                # {'C': C_range, 'kernel': ['linear']},\n",
    "                {'C': C_range, 'gamma': [0.001, 0.0001, 'auto', 'scale'], 'kernel': ['rbf']},\n",
    "                ]\n",
    "    scoring = {'f1_macro'}\n",
    "    clf = svm.SVC()\n",
    "    grid = GridSearchCV(clf,param_grid=param_grid,scoring=scoring,cv=5,refit='f1_macro')\n",
    "    y = bestChdf.iloc[:,-1].values\n",
    "    X = bestChdf.iloc[:,:-1].values\n",
    "    #Split into training and test sets\n",
    "    for train_index, test_index in sss.split(X,y):\n",
    "        X_train, X_test = X[train_index],X[test_index]\n",
    "        y_train, y_test = y[train_index],y[test_index]\n",
    "    #Fit transform on training data\n",
    "    x_99_train = pca99.fit_transform(X_train)\n",
    "    #Fit transform on test data\n",
    "    x_99_test = pca99.transform(X_test)\n",
    "    #Gridsearch\n",
    "    grid99 = grid.fit(x_99_train,y_train)\n",
    "    #Classifiers \n",
    "    clf99 = svm.SVC(**grid99.best_params_)\n",
    "    clf99.fit(x_99_train,y_train)\n",
    "    #y_pred\n",
    "    y_99_pred = clf99.predict(x_99_test)\n",
    "    #F1 Score \n",
    "    f1Score = f1_score(y_test,y_99_pred,pos_label='T')\n",
    "    return f1Score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_f1Scores = [0]*len(sorted_channels)\n",
    "for i in range(1,len(sorted_channels)+1):\n",
    "    bestCh_df = create_bestCh_df(t_auc_df,a_auc_df,t_psd_df,a_psd_df,sorted_channels,i)\n",
    "    channels_f1Scores[i-1] = PCA_CV_SVM(bestCh_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.plot(np.linspace(1,128,128),channels_f1Scores)\n",
    "ax.set_xlabel(\"No. of features, starting from most important\")\n",
    "ax.set_ylabel(\"F1 Score\")\n",
    "ax.set_title(\"Think-read, channel importance using PSD estimates\")\n",
    "# ax.set_xticks(np.linspace(1,128,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Highest F1 score: 0.8769230769230768\n"
    }
   ],
   "source": [
    "maxF1 = max(channels_f1Scores)\n",
    "print(\"Highest F1 score:\",maxF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "No. of channels used to obtain highest F1 score: 108\nChannels used: ['aCh29_t' 'Ch29_t' 'aCh19_t' 'aCh5_t' 'Ch1_a' 'Ch5_t' 'aCh28_a' 'Ch19_t'\n 'aCh27_t' 'Ch22_t' 'aCh1_a' 'Ch1_t' 'aCh28_t' 'aCh20_t' 'aCh22_t'\n 'Ch28_t' 'aCh26_t' 'Ch28_a' 'Ch15_t' 'aCh15_t' 'aCh1_t' 'aCh29_a'\n 'Ch26_t' 'Ch5_a' 'aCh5_a' 'Ch13_t' 'Ch20_t' 'Ch29_a' 'aCh13_t' 'Ch31_t'\n 'aCh17_t' 'Ch17_t' 'aCh16_t' 'aCh31_t' 'Ch27_t' 'aCh21_t' 'aCh18_t'\n 'Ch21_t' 'Ch2_t' 'Ch18_t' 'aCh2_t' 'aCh11_a' 'Ch23_t' 'aCh23_t' 'aCh24_t'\n 'aCh7_t' 'Ch24_t' 'Ch12_t' 'aCh11_t' 'aCh4_a' 'aCh10_t' 'aCh12_t'\n 'Ch11_a' 'Ch11_t' 'Ch10_t' 'aCh30_a' 'Ch16_t' 'aCh30_t' 'aCh12_a'\n 'Ch25_t' 'aCh25_t' 'aCh4_t' 'aCh24_a' 'Ch24_a' 'Ch30_t' 'Ch4_t' 'aCh3_t'\n 'Ch8_t' 'aCh6_t' 'aCh8_t' 'Ch30_a' 'aCh8_a' 'Ch3_t' 'aCh31_a' 'Ch8_a'\n 'Ch9_t' 'aCh9_t' 'Ch6_t' 'Ch26_a' 'Ch12_a' 'Ch4_a' 'Ch31_a' 'Ch10_a'\n 'aCh10_a' 'Ch19_a' 'aCh26_a' 'aCh21_a' 'aCh15_a' 'Ch6_a' 'aCh6_a'\n 'Ch27_a' 'Ch3_a' 'aCh7_a' 'aCh32_t' 'aCh14_a' 'aCh2_a' 'aCh16_a' 'Ch2_a'\n 'Ch14_t' 'aCh20_a' 'Ch16_a' 'Ch7_a' 'Ch14_a' 'aCh3_a' 'Ch22_a' 'aCh14_t'\n 'aCh17_a' 'aCh22_a']\n"
    }
   ],
   "source": [
    "maxF1_channels = sorted_channels[:np.array(channels_f1Scores).argmax()]\n",
    "print(\"No. of channels used to obtain highest F1 score:\", len(maxF1_channels))\n",
    "print(\"Channels used:\",maxF1_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequently used channels \n",
    "maxF1_channelsDict = {}\n",
    "for channel in maxF1_channels:\n",
    "    channel = channel[channel.find(\"Ch\"):channel.find(\"_\")]\n",
    "    if channel not in maxF1_channelsDict.keys():\n",
    "        maxF1_channelsDict[channel] = 1\n",
    "    else:\n",
    "        maxF1_channelsDict[channel]+=1\n",
    "maxF1_channelsDict = {k:v for k,v in sorted(maxF1_channelsDict.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'Ch29': 4, 'Ch5': 4, 'Ch1': 4, 'Ch28': 4, 'Ch22': 4, 'Ch26': 4, 'Ch31': 4, 'Ch16': 4, 'Ch2': 4, 'Ch11': 4, 'Ch24': 4, 'Ch12': 4, 'Ch4': 4, 'Ch10': 4, 'Ch30': 4, 'Ch3': 4, 'Ch8': 4, 'Ch6': 4, 'Ch14': 4, 'Ch19': 3, 'Ch27': 3, 'Ch20': 3, 'Ch15': 3, 'Ch17': 3, 'Ch21': 3, 'Ch7': 3, 'Ch13': 2, 'Ch18': 2, 'Ch23': 2, 'Ch25': 2, 'Ch9': 2, 'Ch32': 1}\n"
    }
   ],
   "source": [
    "print(maxF1_channelsDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(maxF1_channelsDict)),maxF1_channelsDict.values())\n",
    "plt.xticks(range(len(maxF1_channelsDict)),list(maxF1_channelsDict.keys()))\n",
    "plt.title(\"Think-read, Most commonly used channels for highest F1 Score\")\n",
    "plt.ylabel(\"No. of occurence\")\n",
    "plt.xlabel(\"Channels\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discard channel if performs is worse, start from most important channel\n",
    "Dchannels_f1Scores = [0]\n",
    "Dsorted_channels = sorted_channels[:]\n",
    "for i in range(len(sorted_channels)):\n",
    "    no_channels = 1\n",
    "    drop_channels = []\n",
    "    bestCh_df = create_bestCh_df(t_auc_df,a_auc_df,t_psd_df,a_psd_df,Dsorted_channels,no_channels)\n",
    "    current_f1Score = PCA_CV_SVM(bestCh_df)\n",
    "    if current_f1Score>Dchannels_f1Scores[-1]:\n",
    "        Dchannels_f1Scores.append(current_f1Score)\n",
    "        no_channels +=1\n",
    "    else:\n",
    "        Dsorted_channels = np.delete(Dsorted_channels,np.where(Dsorted_channels==sorted_channels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load feature scaled datasets \n",
    "auc_df = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\sampleSize_comparison\\\\featureScaled\\AUC/AUC_df_0.1s.pkl\")\n",
    "psd_df = pd.read_pickle(\"F:\\EEG-data\\\\think-read\\sampleSize_comparison\\\\featureScaled\\PSD/PSD_df_0.1s.pkl\")\n",
    "_,acols = auc_df.shape\n",
    "_,pcols = psd_df.shape\n",
    "\n",
    "t_auc_df = auc_df.iloc[:,:int((acols-1)/2)]\n",
    "a_auc_df = auc_df.iloc[:,int((acols-1)/2):-1]\n",
    "t_psd_df = psd_df.iloc[:,:int((acols-1)/2)]\n",
    "a_psd_df = psd_df.iloc[:,int((acols-1)/2):-1]\n",
    "\n",
    "y_auc = auc_df.iloc[:,-1]\n",
    "y_psd = psd_df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(640, 32)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "t_channels,t_f1_scores = apply_SVM_channel(t_auc_df,t_psd_df)\n",
    "a_channels,a_f1_scores = apply_SVM_channel(a_auc_df,a_psd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(640, 32)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "a_auc_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(640, 65)"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "auc_df.shape"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bit4f00f251aa71407b905d36ad95b25cdd",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}