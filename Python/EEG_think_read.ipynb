{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Thinking and Reading \n",
    "\n",
    "10s - 10s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pickle as pkl\n",
    "import itertools \n",
    "import glob\n",
    "from sklearn import svm \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# %matplotlib inline \n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#Importing raw data files \n",
    "\n",
    "#.csv path\n",
    "csvpath = \"C:/Users/Wu Di/Documents/EEG-analysis/200108-Readings-csv/reading_switching.csv\"\n",
    "\n",
    "#Read .csv files\n",
    "cols_to_use = list(range(4, 36))\n",
    "\n",
    "#Raw dataframes - each channel is a column\n",
    "raw_df = pd.read_csv(csvpath, header=None, usecols=cols_to_use)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bandpass (BP) filter helper functions\n",
    "\n",
    "#Creates butterworth BP filter\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5*fs  # Nyquist frequency, which is half of fs\n",
    "    low = lowcut/nyq  # Digital butterworth filter cutoffs must be normalized to Nyquist frequency\n",
    "    high = highcut/nyq\n",
    "    b, a = signal.butter(order, [low, high], btype=\"bandpass\")\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass(cutFreq,fs,order=5):\n",
    "    nyq = 0.5*fs\n",
    "    cutFreq = cutFreq/nyq\n",
    "    b,a = signal.butter(order,cutFreq,btype=\"lowpass\")\n",
    "    return b,a \n",
    "\n",
    "def butter_highpass(cutFreq,fs,order=5):\n",
    "    nyq = 0.5*fs\n",
    "    cutFreq = cutFreq/nyq\n",
    "    b,a = signal.butter(order,cutFreq,btype=\"highpass\")\n",
    "    return b,a \n",
    "\n",
    "#Applies butterworth BP filter\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "#     filtered_data = signal.lfilter(b, a, data)\n",
    "    filtered_data = signal.filtfilt(b,a,data)\n",
    "    return filtered_data\n",
    "\n",
    "#Applies butterworth lowpass filter\n",
    "def butter_lowpass_filter(data, cutFreq, fs, order=5):\n",
    "    b, a = butter_lowpass(cutFreq,fs,order=5)\n",
    "    filtered_data = signal.filtfilt(b,a,data)\n",
    "    return filtered_data\n",
    "\n",
    "#Applies butterworth lowpass filter\n",
    "def butter_highpass_filter(data, cutFreq, fs, order=5):\n",
    "    b, a = butter_highpass(cutFreq,fs,order=5)\n",
    "    filtered_data = signal.filtfilt(b,a,data)\n",
    "    return filtered_data\n",
    "\n",
    "#Applies butterworth BP filter to Pandas dataframe \n",
    "def bp_filter_df(df, lowcut, highcut, fs, order):\n",
    "    rows, cols = df.shape  # Get no. of rows and cols in df\n",
    "    new_index = range(1, rows+1)\n",
    "    new_cols = range(1, cols+1)\n",
    "    # Create new df with same no. of rows and cols\n",
    "    new_df = pd.DataFrame(index=new_index, columns=new_cols)\n",
    "    # new_df = new_df.fillna(0) #Fill in 0 for all values\n",
    "    for i in range(cols):  # Apply bp filter each column (channel) and saves in new_df\n",
    "        filt_col = butter_bandpass_filter(\n",
    "            df.iloc[:, i].values, lowcut, highcut, fs, order)\n",
    "        new_df[i+1] = filt_col\n",
    "    return new_df\n",
    "\n",
    "#Applies butterworth lowpass filter to Pandas dataframe \n",
    "def lp_filter_df(df, cutFreq, fs, order):\n",
    "    rows, cols = df.shape  # Get no. of rows and cols in df\n",
    "    new_index = range(1, rows+1)\n",
    "    new_cols = range(1, cols+1)\n",
    "    # Create new df with same no. of rows and cols\n",
    "    new_df = pd.DataFrame(index=new_index, columns=new_cols)\n",
    "    # new_df = new_df.fillna(0) #Fill in 0 for all values\n",
    "    for i in range(cols):  # Apply bp filter each column (channel) and saves in new_df\n",
    "        filt_col = butter_lowpass_filter(\n",
    "            df.iloc[:, i].values, cutFreq, fs, order)\n",
    "        new_df[i+1] = filt_col\n",
    "    return new_df\n",
    "\n",
    "#Applies butterworth highpass filter to Pandas dataframe \n",
    "def hp_filter_df(df, cutFreq, fs, order):\n",
    "    rows, cols = df.shape  # Get no. of rows and cols in df\n",
    "    new_index = range(1, rows+1)\n",
    "    new_cols = range(1, cols+1)\n",
    "    # Create new df with same no. of rows and cols\n",
    "    new_df = pd.DataFrame(index=new_index, columns=new_cols)\n",
    "    # new_df = new_df.fillna(0) #Fill in 0 for all values\n",
    "    for i in range(cols):  # Apply bp filter each column (channel) and saves in new_df\n",
    "        filt_col = butter_highpass_filter(\n",
    "            df.iloc[:, i].values, cutFreq, fs, order)\n",
    "        new_df[i+1] = filt_col\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(3840, 32)\n(3840, 32)\n"
    }
   ],
   "source": [
    "#Apply BP filtering to raw dataframes\n",
    "def filt_freq_bands(df,fs):\n",
    "    delta = lp_filter_df(df, 4.5, fs, 10) \n",
    "    theta = bp_filter_df(df, 3.5, 8.5, fs, 6)\n",
    "    alpha = bp_filter_df(df, 7.5, 12.5, fs, 8)\n",
    "    beta = bp_filter_df(df, 11.5, 30.5, fs, 16)\n",
    "    gamma = hp_filter_df(df, 29.5, fs, 50)\n",
    "    return [delta, theta, alpha, beta, gamma]\n",
    "\n",
    "fs = 128\n",
    "# order = 6\n",
    "\n",
    "all_bands_list = filt_freq_bands(raw_df,fs)\n",
    "\n",
    "#Split into thinking and counting data frames\n",
    "think_index_list = []\n",
    "count_index_list = []\n",
    "\n",
    "for i in range(6):\n",
    "    if i%2==0:\n",
    "        think_index_list+=(list(range(1280*i,1280*(i+1))))\n",
    "    else:\n",
    "        count_index_list+=(list(range(1280*i,1280*(i+1))))\n",
    "\n",
    "think_bands_list = []\n",
    "count_bands_list = []\n",
    "\n",
    "for i in range(len(all_bands_list)):\n",
    "    df = all_bands_list[i].iloc[0:1280*6]\n",
    "    df_list = np.vsplit(df,6)\n",
    "    think_df = pd.DataFrame(np.vstack((df_list[0],df_list[2],df_list[4])))\n",
    "    think_bands_list.append(think_df)\n",
    "    count_df = pd.DataFrame(np.vstack((df_list[1],df_list[3],df_list[5])))\n",
    "    count_bands_list.append(count_df)\n",
    "\n",
    "print(think_bands_list[0].shape)\n",
    "print(count_bands_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total no. of recorded samples: 3840\nSample length: 12\nTotal no. of samples: 320\nLength of df_split_list: 320\nShape of each element in df_split_list: (12, 32)\nTotal no. of recorded samples: 3840\nSample length: 12\nTotal no. of samples: 320\nLength of df_split_list: 320\nShape of each element in df_split_list: (12, 32)\nTotal no. of recorded samples: 3840\nSample length: 12\nTotal no. of samples: 320\nLength of df_split_list: 320\nShape of each element in df_split_list: (12, 32)\nTotal no. of recorded samples: 3840\nSample length: 12\nTotal no. of samples: 320\nLength of df_split_list: 320\nShape of each element in df_split_list: (12, 32)\nTotal no. of recorded samples: 3840\nSample length: 12\nTotal no. of samples: 320\nLength of df_split_list: 320\nShape of each element in df_split_list: (12, 32)\nLength of bands_list: 5\nLength of df_list_rFE: 5\nShape of each dataframe in df_list_rFE: (320, 32)\nTotal no. of recorded samples: 3840\nSample length: 12\nTotal no. of samples: 320\nLength of df_split_list: 320\nShape of each element in df_split_list: (12, 32)\nTotal no. of recorded samples: 3840\nSample length: 12\nTotal no. of samples: 320\nLength of df_split_list: 320\nShape of each element in df_split_list: (12, 32)\nTotal no. of recorded samples: 3840\nSample length: 12\nTotal no. of samples: 320\nLength of df_split_list: 320\nShape of each element in df_split_list: (12, 32)\nTotal no. of recorded samples: 3840\nSample length: 12\nTotal no. of samples: 320\nLength of df_split_list: 320\nShape of each element in df_split_list: (12, 32)\nTotal no. of recorded samples: 3840\nSample length: 12\nTotal no. of samples: 320\nLength of df_split_list: 320\nShape of each element in df_split_list: (12, 32)\nLength of bands_list: 5\nLength of df_list_rFE: 5\nShape of each dataframe in df_list_rFE: (320, 32)\n"
    }
   ],
   "source": [
    "#Split filtered dataframes into samples\n",
    "fs = 128 #sampling freq\n",
    "sample_t = 0.1 #sample time length in seconds\n",
    "\n",
    "#Splits a single dataframe into list of equally sized arrays\n",
    "#Each element in list is nx32 array, where n= sample length \n",
    "def split_df(df,fs,sample_t,check=False):\n",
    "    rows,_ = df.shape #get no. of rows\n",
    "    sample_len = int(sample_t*fs) #find no. of recorded samples required for each sample time length\n",
    "    Ns = int(rows/sample_len) #find total no. of samples\n",
    "    df_cut = df.loc[:Ns*sample_len] #truncate dataframe to exact multiple of sample length\n",
    "    df_split_list = np.vsplit(df_cut,Ns) #split dataframe row-wise, returns a list\n",
    "    \n",
    "    if check:\n",
    "        print(\"Total no. of recorded samples: \"+str(rows))\n",
    "        print(\"Sample length: \"+str(sample_len))\n",
    "        print(\"Total no. of samples: \"+str(Ns))\n",
    "        print(\"Length of df_split_list: \"+str(len(df_split_list)))\n",
    "        \n",
    "        if all(isinstance(x.shape,tuple) for x in df_split_list):\n",
    "            print(\"Shape of each element in df_split_list: \"+str(df_split_list[0].shape))\n",
    "        else:\n",
    "            print(\"Shapes are wrong.\")\n",
    "            for x in df_split_list:\n",
    "                print(x.shape)\n",
    "    return df_split_list,Ns\n",
    "\n",
    "#Apply split_df() function to list of dataframes, reshape dataframe such that each element is an array \n",
    "#for the appropriate sample time length \n",
    "def split_bands_list(bands_list,fs,sample_t,check=False,checkSD=False):\n",
    "    df_list_rFE = [0]*len(bands_list) #dataframes list ready for feature extraction \n",
    "    for df_no in range(len(bands_list)):\n",
    "        df_split_list,Ns = split_df(bands_list[df_no],fs,sample_t,check=checkSD)\n",
    "        list_of_series = [0]*Ns\n",
    "        for i in range(len(df_split_list)):\n",
    "            #New dataframe will have shape Nsx32, each element is a 1xsample_len array \n",
    "            new_row = [0]*32 \n",
    "            #Each df_split_list[i] is a dataframe\n",
    "            for j in range(len(df_split_list[i].columns)):\n",
    "                new_row[j] = df_split_list[i].iloc[:,j].values \n",
    "            list_of_series[i] = new_row\n",
    "        df_list_rFE[df_no] = pd.DataFrame(list_of_series)\n",
    "    if check:\n",
    "        print(\"Length of bands_list: \"+str(len(bands_list)))\n",
    "        print(\"Length of df_list_rFE: \"+str(len(df_list_rFE)))\n",
    "        if (all(isinstance(x.shape,tuple) for x in df_list_rFE)) and (Ns==len(df_list_rFE[0].index)):\n",
    "            print(\"Shape of each dataframe in df_list_rFE: \"+str(df_list_rFE[0].shape))\n",
    "    return df_list_rFE\n",
    "            \n",
    "T_bands_split_list = split_bands_list(think_bands_list,fs,sample_t,check=True,checkSD=True)\n",
    "C_bands_split_list = split_bands_list(count_bands_list,fs,sample_t,check=True,checkSD=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(320, 32)\n(320, 32)\n"
    }
   ],
   "source": [
    "#Apply Welch's method for each item in the dataframes\n",
    "\n",
    "def welch_bands_split_list(bands_list,fs,win,check=False):\n",
    "    #fs -sampling freq, win - number of recorded samples in window\n",
    "    #Using this method, freq resolution = 1/t = fs/N (N=no.of points in windows)\n",
    "    PSD_df_list = [0]*len(bands_list)\n",
    "    for df_no in range(len(bands_list)):\n",
    "        #Every item in the dataframe is mapped to the PSD estimates\n",
    "        PSD_df_list[df_no] = bands_list[df_no].applymap(lambda x: signal.welch(x,fs,nperseg=win)[1])\n",
    "    #Frequency axis for plotting, samee for all\n",
    "    freqs,_ = signal.welch(bands_list[0].iloc[0,0],fs,nperseg=win)\n",
    "    \n",
    "    if check:\n",
    "        if all(isinstance(x.shape,tuple) for x in PSD_df_list):\n",
    "            print(PSD_df_list[0].shape)\n",
    "#         if len(PSD_df_list[0].iloc[0,0]) == len(PSD_df_list[1].iloc[0,0]) == len(PSD_df_list[2].iloc[0,0]) == len(PSD_df_list[3].iloc[0,0]):\n",
    "#             len(PSD_df_list[0].iloc[0,0]\n",
    "    \n",
    "    return freqs, PSD_df_list\n",
    "\n",
    "fs = 128 \n",
    "win = 6 #Half of sample length (12)\n",
    "\n",
    "#Get PSD estimates, freqs is the same for all\n",
    "freqs, T_PSD_df_list = welch_bands_split_list(T_bands_split_list,fs,win,check=True)\n",
    "_, C_PSD_df_list = welch_bands_split_list(C_bands_split_list,fs,win,check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create features from PSD \n",
    "\n",
    "def get_AUC_PSD_df_list(PSD_df_list):\n",
    "    AUC_PSD_df_list = [0]*len(PSD_df_list)\n",
    "    for df_no in range(len(PSD_df_list)):\n",
    "        AUC_PSD_df_list[df_no] = PSD_df_list[df_no].applymap(lambda x: np.trapz(x))\n",
    "    return AUC_PSD_df_list\n",
    "\n",
    "T_AUC_PSD_df_list = get_AUC_PSD_df_list(T_PSD_df_list)\n",
    "C_AUC_PSD_df_list = get_AUC_PSD_df_list(C_PSD_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(320, 128)\n(320, 128)\n"
    }
   ],
   "source": [
    "#Expand all lists in cells to their own variables\n",
    "\n",
    "def expand_PSD_df_list(PSD_df_list):\n",
    "    e_PSD_df_list = [0]*len(PSD_df_list)\n",
    "    for df_no in range(len(PSD_df_list)):\n",
    "        #e_PSD_df_cols_list will be used to create new dataframe\n",
    "        no_PSD = len(PSD_df_list[0].iloc[0,0])\n",
    "        e_PSD_df_cols_list = [0]*32\n",
    "        \n",
    "        for channel in range(len(PSD_df_list[df_no].columns)):\n",
    "            #Expand each column into its own dataframe\n",
    "            new_col = PSD_df_list[df_no][channel].apply(pd.Series)\n",
    "            #Rename every variable in the new column\n",
    "            new_col = new_col.rename(columns = lambda x: \"Ch\"+str(channel+1)+'_'+str(np.linspace(0,64,no_PSD)[x]))\n",
    "            #Add new_col to cols_list\n",
    "            e_PSD_df_cols_list[channel] = new_col\n",
    "        \n",
    "        #Create new dataframe\n",
    "        e_PSD_df = pd.concat(e_PSD_df_cols_list, axis=1)\n",
    "        \n",
    "        #Add to list\n",
    "        e_PSD_df_list[df_no] = e_PSD_df\n",
    "    return e_PSD_df_list \n",
    "\n",
    "T_e_PSD_df_list = expand_PSD_df_list(T_PSD_df_list)\n",
    "C_e_PSD_df_list = expand_PSD_df_list(C_PSD_df_list)\n",
    "\n",
    "print(T_e_PSD_df_list[0].shape)\n",
    "print(C_e_PSD_df_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(0,), (1,), (2,), (3,), (4,), (0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4), (0, 1, 2), (0, 1, 3), (0, 1, 4), (0, 2, 3), (0, 2, 4), (0, 3, 4), (1, 2, 3), (1, 2, 4), (1, 3, 4), (2, 3, 4), (0, 1, 2, 3), (0, 1, 2, 4), (0, 1, 3, 4), (0, 2, 3, 4), (1, 2, 3, 4), (0, 1, 2, 3, 4)]\n<class 'tuple'>\n31\n"
    }
   ],
   "source": [
    "#Create datasets with all possible combinations of frequency bands\n",
    "\n",
    "indices = [0,1,2,3,4] #indices representing delta, theta, alpha, beta, gamma bands in order\n",
    "\n",
    "def all_combinations(any_list):\n",
    "    return itertools.chain.from_iterable(\n",
    "        itertools.combinations(any_list, i + 1)\n",
    "        for i in range(len(any_list)))\n",
    "\n",
    "combos = list(all_combinations(indices)) #2^(len(indices))-1 combinations\n",
    "\n",
    "print(combos)\n",
    "print(type(combos[0]))\n",
    "print(len(combos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "31\n(320, 800)\n31\n(320, 800)\n"
    }
   ],
   "source": [
    "#Concatenate to form all possible dataframes (PSD and AUC features)\n",
    "\n",
    "#Only PSD features\n",
    "#Only AUC features \n",
    "#Both PSD and AUC features \n",
    "\n",
    "def get_1F_combos_df_list(e_PSD_df_list,combos): \n",
    "    #single feature\n",
    "    combos_df_list = [0]*len(combos)\n",
    "    for i in range(len(combos)):\n",
    "        concat_list = [e_PSD_df_list[x] for x in combos[i]]\n",
    "        combos_df_list[i] = pd.concat(concat_list,axis=1)\n",
    "    return combos_df_list\n",
    "\n",
    "def get_2F_combos_df_list(PSD_df_list,AUC_df_list,combos):\n",
    "    #Two features\n",
    "    combos_df_list = [0]*len(combos)\n",
    "    for i in range(len(combos)):\n",
    "        psd_list = [PSD_df_list[x] for x in combos[i]]\n",
    "        auc_list = [AUC_df_list[x] for x in combos[i]]\n",
    "        concat_list = psd_list + auc_list \n",
    "        combos_df_list[i] = pd.concat(concat_list,axis=1)\n",
    "    return combos_df_list\n",
    "\n",
    "#List of dataframes with only PSD features\n",
    "T_PSD_combos_df_list = get_1F_combos_df_list(T_e_PSD_df_list,combos)\n",
    "C_PSD_combos_df_list = get_1F_combos_df_list(C_e_PSD_df_list,combos)\n",
    "\n",
    "#List of dataframes with only AUC features\n",
    "T_AUC_combos_df_list = get_1F_combos_df_list(T_AUC_PSD_df_list,combos)\n",
    "C_AUC_combos_df_list = get_1F_combos_df_list(C_AUC_PSD_df_list,combos)\n",
    "\n",
    "#List of dataframes with both features\n",
    "T_2F_combos_df_list = get_2F_combos_df_list(T_e_PSD_df_list,T_AUC_PSD_df_list,combos)\n",
    "C_2F_combos_df_list = get_2F_combos_df_list(C_e_PSD_df_list,C_AUC_PSD_df_list,combos)\n",
    "\n",
    "print(len(T_2F_combos_df_list))\n",
    "print(C_2F_combos_df_list[30].shape)\n",
    "print(len(C_2F_combos_df_list))\n",
    "print(C_2F_combos_df_list[30].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "31\n31\n"
    }
   ],
   "source": [
    "#Append action type columns to all dataframes \n",
    "\n",
    "def add_music_col(df_list,music_type):\n",
    "    new_list = [0]*len(df_list)\n",
    "    for i in range(len(df_list)):\n",
    "        new_df = df_list[i][:]\n",
    "        new_df['Action'] = pd.Series(music_type,index=df_list[i].index) #add new column\n",
    "        new_list[i] = new_df\n",
    "    return new_list\n",
    "\n",
    "#Lists\n",
    "#List of dataframes with only PSD features\n",
    "T_PSD_combosA_df_list = add_music_col(T_PSD_combos_df_list,'T')\n",
    "C_PSD_combosA_df_list = add_music_col(C_PSD_combos_df_list,'R')\n",
    "\n",
    "# #List of dataframes with only AUC features\n",
    "T_AUC_combosA_df_list = add_music_col(T_AUC_combos_df_list,'T')\n",
    "C_AUC_combosA_df_list = add_music_col(C_AUC_combos_df_list,'R')\n",
    "\n",
    "# #List of dataframes with both features\n",
    "T_2F_combosA_df_list = add_music_col(T_2F_combos_df_list,'T')\n",
    "C_2F_combosA_df_list = add_music_col(C_2F_combos_df_list,'R')\n",
    "\n",
    "print(len(T_PSD_combosA_df_list))\n",
    "print(len(C_PSD_combosA_df_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "31\n31\n31\n"
    }
   ],
   "source": [
    "#Combine different class types to form full datasets\n",
    "\n",
    "#Create list of strings representing combinations of frequency bands to append to saved files\n",
    "def get_combo_strings(combos):\n",
    "    combo_strings = []\n",
    "    for combo in combos:\n",
    "        strings = [str(x) for x in combo]\n",
    "        string = ''\n",
    "        for i in range(len(strings)):\n",
    "            string += strings[i]\n",
    "        combo_strings.append(string)\n",
    "    return combo_strings\n",
    "combo_strings = get_combo_strings(combos)\n",
    "\n",
    "def concatSave_df_list(concat_list,T_df_list,C_df_list,combo_strings,filename,savedir,save=False):\n",
    "    if len(C_df_list) == len(T_df_list) :\n",
    "        for i in range(len(C_df_list)):\n",
    "            new_df = pd.concat([T_df_list[i],C_df_list[i]],axis=0)\n",
    "            concat_list.append(new_df)\n",
    "            # print(len(concat_list))\n",
    "            if save:\n",
    "                savepath = savedir+filename+'_'+ combo_strings[i]+'.pkl'\n",
    "                #Save to external HDD as pkl files \n",
    "                new_df.to_pickle(savepath)\n",
    "    else:\n",
    "        print(\"Lists are of unequal lengths.\")\n",
    "\n",
    "\n",
    "PSDsavedir = 'F:\\EEG-data\\\\read-count\\\\featureExtraction/PSD_only/'\n",
    "AUCsavedir = 'F:\\EEG-data\\\\read-count\\\\featureExtraction/AUC_only/'\n",
    "AUC_PSDsavedir = 'F:\\EEG-data\\\\read-count\\\\featureExtraction/AUC_PSD/'\n",
    "\n",
    "PSD_filename = 'PSD_df'\n",
    "AUC_filename = 'AUC_df'\n",
    "AUC_PSDfilename = 'AUC_PSD_df'\n",
    "\n",
    "PSD_combosA_df_list = []\n",
    "AUC_combosA_df_list = []\n",
    "AUC_PSD_combosA_df_list =[]\n",
    "#\"combosA\" means \"Action\" column has been added\n",
    "#List of dataframes with only PSD features\n",
    "concatSave_df_list(PSD_combosA_df_list,T_PSD_combosA_df_list,C_PSD_combosA_df_list, combo_strings,PSD_filename,PSDsavedir)\n",
    "\n",
    "\n",
    "#List of dataframes with only AUC features\n",
    "concatSave_df_list(AUC_combosA_df_list,T_AUC_combosA_df_list,C_AUC_combosA_df_list, combo_strings,AUC_filename,AUCsavedir)\n",
    "\n",
    "#List of dataframes with both features\n",
    "concatSave_df_list(AUC_PSD_combosA_df_list,T_2F_combosA_df_list,C_2F_combosA_df_list, combo_strings,AUC_PSDfilename,AUC_PSDsavedir)\n",
    "\n",
    "\n",
    "print(len(PSD_combosA_df_list))\n",
    "print(len(AUC_combosA_df_list))\n",
    "print(len(AUC_PSD_combosA_df_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for feature scaling\n",
    "PSDsavedir = 'F:\\EEG-data\\\\think-read\\\\featureExtraction/PSD_only/'\n",
    "AUCsavedir = 'F:\\EEG-data\\\\think-read\\\\featureExtraction/AUC_only/'\n",
    "AUC_PSDsavedir = 'F:\\EEG-data\\\\think-read\\\\featureExtraction/AUC_PSD/'\n",
    "\n",
    "PSD_filename = 'PSD_df'\n",
    "AUC_filename = 'AUC_df'\n",
    "AUC_PSDfilename = 'AUC_PSD_df'\n",
    "\n",
    "def featureScaling_df(combosA_df_list,combo_strings,filebase,savedir):\n",
    "    sc = StandardScaler()\n",
    "    for i in range(len(combosA_df_list)):\n",
    "        filename = filebase+'_'+ combo_strings[i]+'.pkl'\n",
    "        df = combosA_df_list[i]\n",
    "        cols = df.columns\n",
    "        data = df.iloc[:,0:-1]\n",
    "        class_type = df.iloc[:,-1].values\n",
    "        class_type = np.reshape(class_type,(len(class_type),1))\n",
    "        scaled_data = sc.fit_transform(data)\n",
    "        new_df = pd.DataFrame(np.hstack((scaled_data,class_type)),columns=cols)\n",
    "        savefile = savedir+filename\n",
    "        new_df.to_pickle(savefile)\n",
    "\n",
    "featureScaling_df(PSD_combosA_df_list,combo_strings,PSD_filename,PSDsavedir)\n",
    "featureScaling_df(AUC_combosA_df_list,combo_strings,AUC_filename,AUCsavedir)\n",
    "featureScaling_df(AUC_PSD_combosA_df_list,combo_strings,AUC_PSDfilename,AUC_PSDsavedir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM, Cross Validation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature scaled datasets\n",
    "loaddir = 'F:\\EEG-data\\\\think-read\\\\featureExtraction/'\n",
    "loadAUC_dir = loaddir+'AUC_only/'\n",
    "loadPSD_dir = loaddir+'PSD_only/'\n",
    "loadAUC_PSD_dir = loaddir+'AUC_PSD/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-2a798018b6d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[0msaveAUC_PSD_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msavedir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'AUC_PSD/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m \u001b[0mapply_PCA_CV_SVM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloadAUC_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msaveAUC_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[0mapply_PCA_CV_SVM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloadPSD_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msavePSD_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[0mapply_PCA_CV_SVM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloadAUC_PSD_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msaveAUC_PSD_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-2a798018b6d3>\u001b[0m in \u001b[0;36mapply_PCA_CV_SVM\u001b[1;34m(loaddir, savedir)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m#Gridsearch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mgrid99\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_99_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mgrid1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;31m#Get number of principal components\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcols99\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_99_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    708\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    687\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 689\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    256\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cross-validation for SVM \n",
    "def apply_PCA_CV_SVM(loaddir,savedir):\n",
    "    files = glob.glob(loaddir+'*.pkl')\n",
    "    sss = StratifiedShuffleSplit(n_splits=5,test_size=0.2,random_state=0)\n",
    "    testdir = savedir+'testResults/'\n",
    "    bestF1 = {'F1 Score':0,'dataset':'','params':''}\n",
    "    test_results = {'F1 Score':[],'dataset':[],'params':[]}\n",
    "    #Create PCA instances\n",
    "    pca99 = PCA(n_components=0.99, svd_solver='full')\n",
    "    #Instantiate SVM gridsearch\n",
    "    C_range = [1,3,10,30,100,300,1000]\n",
    "    param_grid= [\n",
    "                # {'C': C_range, 'kernel': ['linear']},\n",
    "                {'C': C_range, 'gamma': [0.001, 0.0001, 'auto', 'scale'], 'kernel': ['rbf']},\n",
    "                ]\n",
    "    scoring = {'accuracy','f1_macro'}\n",
    "    clf = svm.SVC()\n",
    "    grid = GridSearchCV(clf,param_grid=param_grid,scoring=scoring,cv=5,refit='f1_macro')\n",
    "    #Apply PCA and Gridsearch\n",
    "    for file in files:\n",
    "        #Create filenames for saving\n",
    "        cv99_filename = file.split(\"\\\\\")[-1].rstrip('.pkl') +'_PCA99_CV_results.pkl'\n",
    "        cv1_filename = file.split(\"\\\\\")[-1].rstrip('.pkl') +'_CV_results.pkl'\n",
    "        df = pd.read_pickle(file)\n",
    "        X = df.iloc[:,:-1].values\n",
    "        y = df.iloc[:,-1].values\n",
    "        #Split into training and test sets\n",
    "        for train_index, test_index in sss.split(X,y):\n",
    "            X_train, X_test = X[train_index],X[test_index]\n",
    "            y_train, y_test = y[train_index],y[test_index]\n",
    "        #Fit transform on training data\n",
    "        x_99_train = pca99.fit_transform(X_train)\n",
    "        #Fit transform on test data\n",
    "        x_99_test = pca99.transform(X_test)\n",
    "        #Gridsearch\n",
    "        grid99 = grid.fit(x_99_train,y_train)\n",
    "        grid1 = grid.fit(X_train,y_train)\n",
    "        #Get number of principal components\n",
    "        _,cols99 = x_99_train.shape\n",
    "        #Get results\n",
    "        results_df99 = pd.DataFrame(grid99.cv_results_)\n",
    "        results_df1 = pd.DataFrame(grid1.cv_results_)\n",
    "        #Append number of principal components\n",
    "        results_df99['No. of PC'] = [cols99]*len(results_df99.index) \n",
    "        #Save results\n",
    "        # results_df99.to_pickle(savedir+cv99_filename)\n",
    "        # results_df1.to_pickle(savedir+cv1_filename)\n",
    "        #Export as csv \n",
    "        # results_df99.to_csv(savedir+cv99_filename+'.csv')\n",
    "        #Fit SVM on best parameters \n",
    "        best_params99 = results_df99.loc[results_df99['rank_test_f1_macro'].idxmin()]['params']\n",
    "        best_params1 = results_df1.loc[results_df1['rank_test_f1_macro'].idxmin()]['params']\n",
    "        \n",
    "        clf99 = svm.SVC(**best_params99)\n",
    "        clf99.fit(x_99_train,y_train)\n",
    "        y_99_pred = clf99.predict(x_99_test)\n",
    "\n",
    "        clf1 = svm.SVC(**best_params1)\n",
    "        clf1.fit(X_train,y_train)\n",
    "        y_1_pred = clf1.predict(X_test)\n",
    "\n",
    "        y_pred_list = [y_99_pred,y_1_pred]\n",
    "        x_test_list = [x_99_test,X_test]\n",
    "        clf_list = [clf99,clf1]\n",
    "        best_params_list =[best_params99,best_params1]\n",
    "\n",
    "        #Generate and save classification report, macro-avg f1 score, confusion matrix\n",
    "        \n",
    "        target_names = ['C','T']\n",
    "        names_list = ['99','1']\n",
    "        for i in range(len(y_pred_list)):\n",
    "            df_name = file.split(\"\\\\\")[-1].rstrip('.pkl')+'_'+names_list[i]\n",
    "            report_name = file.split(\"\\\\\")[-1].rstrip('.pkl') + '_'+names_list[i]+'_PCA_CV_SVM_clf_report'\n",
    "            cm_name = file.split(\"\\\\\")[-1].rstrip('.pkl') + '_'+names_list[i]+'_PCA_CV_SVM_ConfusionMatrix'\n",
    "            ##Classification report \n",
    "            report = classification_report(y_test, y_pred_list[i], target_names=target_names, output_dict=True)\n",
    "            report_df = pd.DataFrame(report).transpose()\n",
    "            report_df.to_pickle(savedir+report_name+'.pkl')\n",
    "            ##Macro-avg f1 score \n",
    "            f1Score = report['macro avg']['f1-score']\n",
    "            test_results['F1 Score'].append(f1Score)\n",
    "            test_results['dataset'].append(df_name)\n",
    "            test_results['params'].append(best_params_list[i])\n",
    "            test_results_df = pd.DataFrame(test_results)\n",
    "            if bestF1['F1 Score']<f1Score:\n",
    "                bestF1['F1 Score'] = f1Score\n",
    "                bestF1['dataset'] = df_name\n",
    "                bestF1['params'] = str(best_params_list[i])\n",
    "            ##Confusion matrix\n",
    "            fig,ax = plt.subplots()\n",
    "            ax.set_title(df_name+' CM')\n",
    "            plot_confusion_matrix(clf_list[i],x_test_list[i],y_test,labels=target_names,ax=ax,normalize='true')\n",
    "            plt.savefig(testdir+df_name+'.png')\n",
    "            plt.close()\n",
    "\n",
    "    bestF1_df = pd.DataFrame(bestF1,index=[0],columns=['F1 Score','dataset','params'])\n",
    "    bestF1_df.to_csv(testdir+bestF1['dataset']+'.csv')\n",
    "    test_results_df.to_csv(testdir+'test_results.csv')\n",
    "    test_results_df.to_csv(testdir+'test_results.pkl')\n",
    "        \n",
    "\n",
    "        \n",
    "savedir = 'F:\\EEG-data\\\\think-read\\PCA_CV_results/'\n",
    "saveAUC_dir = savedir+'AUC_only/'\n",
    "savePSD_dir = savedir+'PSD_only/'\n",
    "saveAUC_PSD_dir = savedir+'AUC_PSD/'\n",
    "\n",
    "apply_PCA_CV_SVM(loadAUC_dir,saveAUC_dir)\n",
    "apply_PCA_CV_SVM(loadPSD_dir,savePSD_dir)\n",
    "apply_PCA_CV_SVM(loadAUC_PSD_dir,saveAUC_PSD_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bit4f00f251aa71407b905d36ad95b25cdd",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}